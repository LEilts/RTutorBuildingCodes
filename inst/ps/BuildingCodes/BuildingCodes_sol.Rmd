# Effects of Building Codes on Energy Consumption

Master Thesis - "Effects of Building Codes on Energy Consumption. An Empirical Investigation with R."  
<br> Author:  Lisa Eilts  
  
#< ignore
```{r ""}
library(restorepoint)
# facilitates error detection
# set.restore.point.options(display.restore.point=TRUE)

library(RTutor)
library(yaml)
#library(restorepoint)
setwd("C:/Users/admin/Documents/Master/Masterarbeit/final version")
ps.name = "BuildingCodes"; sol.file = paste0(ps.name,"_sol.Rmd")
libs = c("foreign","dplyr","ggplot2", "tidyr","gridExtra", "scales", "lfe", "texreg") # character vector of all packages you load in the problem set
name.rmd.chunks(sol.file) # set auto chunk names in this file
create.ps(sol.file=sol.file, ps.name=ps.name, user.name=NULL,libs=libs, stop.when.finished=FALSE,
          addons = "quiz", var.txt.file = "variables.txt")
show.shiny.ps(ps.name, load.sav=FALSE,  sample.solution=TRUE, is.solved=FALSE, catch.errors=TRUE,
              launch.browser=TRUE)
stop.without.error()
```
#>

Hello! Welcome to this interactive RTutor problem set which is part of my master thesis at the University of Ulm. Great that you found it. Now you can work on this problem set to practise a little bit in R programming. Additionally you can find out whether building energy codes are an effective way at saving energy in practice and you learn some facts about a few econometric models. Just try and after some short introduction you can directly start with the problem set. 


  First I will give you some information about the content of this problem set, second I will shortly explain how to work through the problem set and third you will see the structure of the problem set.

### Paper
  Policymakers introduce many laws and regulations to reduce the energy consumption but do they really help in practice? Or had their establishment no effect in daily energy use?
  <!-- more details, p.1 of paper -->
<br>An answer to this question using the example of building energy codes in Florida is given by Grant D. Jacobsen and Matthew J. Kotchen in their paper with the title **"ARE BUILDING CODES EFFECTIVE AT SAVING ENERGY? EVIDENCE FROM RESIDENTIAL BILLING DATA IN FLORIDA"** (2010). You can download the article from the following page: http://www.nber.org/papers/w16194 and the Stata code is available via the following link https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/20533.
<br> The authors collected monthly utility billing data for electricity and natural gas consumption of households in Gainesville which is a city in the northern part of Florida. In the year 2002, the Florida Building Commission implemented a further tightening of the building energy code, which was initially introduced in 1978. The energy code states a minimal standard for the overall energy efficiency of a newly built residence compared to a baseline home. The new home has to be more or as efficient as the baseline home. The characteristics of the baseline home have changed along with the tightened regulation in 2002.
  The aim of the paper is to compare the residences constructed before and after the code change to get to know how the code change affects the actual energy consumption. Therefore Jacobsen and Kotchen used two empirical strategies:
  <!-- First an "analysis of consumption levels after controlling for differences in observable characteristics" (p. 3) and second "difference-in-differences estimates of the responsiveness of energy consumption to variability in weather" (p. 3). In this problem set we will replicate the authors' results in R and get to know the empirical strategies. -->

### Problem set
  The following abstract will provide you some basic knowledge you need for solving the problem set. In general it is not necessary to solve the exercises in the given order but it is recommended to do so because of the contentual structure. Another reason is that later exercises receive the knowledge (e.g. of commands) of earlier exercises. But within an exercise you have to solve the tasks in the given order. (Only tasks with an explicit remark can be skipped.) Every time you start a new exercise you have to click on the button `edit`. After that you have the possibility to enter your code in the chunk. To check whether your solution is right just click on the button `check`. If you have difficulties while solving the task you can press `hint` and you get some advice. To get a sample solution just click on the `solution` button.


## Exercise Content

**Exercise 1: Overview of the data and summary statistics**
<br>**Exercise 1.1:** Energy consumption and effective year built
<br>**Exercise 1.2:** Housing characteristics
<br>**Exercise 1.3:** Weather variables

**Exercise 2: Building code change in more detail**
<br>**Exercise 2.1:** Analysis of the mean values
<br>**Exercise 2.2:** Statistical analysis by using a two sample t-test

**Exercise 3: Empirical analysis: pre- and post-code-change comparisons**
<br>**Exercise 3.1:** Annual differences
<br>**Exercise 3.2:** Monthly differences
<br>**Exercise 3.3:** Possible limitations

**Exercise 4: Empirical analysis: difference-in-differences estimation**

**Exercise 5: Conclusion**

**Exercise 6: References**


## Exercise 1 -- Overview of the data and summary statistics

This problem set analyses the effects of building codes on energy consumption, to be exact the electricity and natural gas consumption of households in Gainesville, Florida. We want to know if the code change affects the actual energy consumption and leads to a reduction of it.

The first exercise will introduce you to the topic and mainly will provide an insight into the collected data. We use monthly billing data of several households in Gainesville. We will analyse the data in more detail and calculate some important values to get more information about the residences and the energy consumption. Moreover, we will draw some plots to visualise the results. 

Now let's start with the analysis. To be able to take a look at the data we first have to read in the data set. To do so we need the function `read.dta()` because the original data is available as Stata code with file name extension "???.dta". Write the name of the data frame you want to load in the brackets of the function and put the file name in quotation marks. Additionally we can assign the data frame with a specific name, e.g. `dat` which enables us to work with the data frame later on. In total, the command should have the following form: `dat = read.dta("???")`. The function `read.dta()`is part of the package `foreign`. You can get more information about this package on https://cran.r-project.org/web/packages/foreign/foreign.pdf. Therefore, first of all we have to load this package with the command `library(foreign)`.

**Task:** Use the function `read.dta()` to read in the data set which is called `billingdata.dta`. Before you do this you have to load the package `foreign` with the command `library()`. Enter your command and press `check`. If you need help click on the button `hint`.
```{r "1"}
#< task
# load the package foreign
# enter your code here...
#>
library(foreign)
#< hint
display("You want to load the package foreign. Use the command library() and type the name of the package inside. Then press check afterwards.")
#>
```

Now the required package is loaded and you can read in the data set `billingdata.dta`. Assign it with the name `dat`.
```{r "1__2"}
#< task
# enter your code here...
#>
dat = read.dta("billingdata.dta")

#< hint
display("Your command should have the following form: dat = read.dta(\"???\"). Replace the ??? by the name of the data set.")
#>
```

#< award "Sucessfully started"
Great! You have solved your first task correctly. Now you know how to import a data set.

While going on and solving the problem set you can win some more awards. To look at all your received awards write the command `awards()` in the code chunk and then press the button `run chunk`.
#>

Now we take a closer look at the data. This step is really important because you have to be familiar with the data to do further analyses and calculations. First take a look at the column and row names and make sure you understand their meaning. Then analyse some values and look for exceptionalities. While working with data you always have to be careful because there can probably occur errors or there may be missing data. Additionally you should visualise some of the data for a better illustration (according to Kennedy (2008), p. 363f.). 
In the following steps you can use some helpful commands to get more familiar with the data. In the following exercises 3.1 to 3.3 we will also plot some of the data for a better understanding. 

### Get familiar with the data 

In our data set `dat` each row represents one observations. So the total number of rows equals to the number of all included observations.

**Task:** Find out the total number of observations of our data set `dat`. One single command is enough here. Press `hint` if you need some advice.
```{r "1__3"}
#< task
# enter your code here...
#>
nrow(dat)
#< hint
display("You only have to count the number of rows of dat. Therefore, use the function nrow(). Put the name of the data set inside.")
#>
```

The number of observation is that large because we have monthly billing data from the years $2004$ to $2006$ and we have $2239$ different residences, indicated by the variable `home_id`.

More interesting are the different columns of our data set. In the following code box we get to know how the different columns are named and implicitly how many columns there are in our data frame `dat`.  

**Task:** Use the command `colnames()` to return all names of the columns of the data frame `dat`. Press `check` afterwards.
```{r "1__4"}
#< task
# enter your code here...
#>
colnames(dat)
```

If you want get some more information about the data frame you can use the command `head()`. It returns the column names together with the six first entries of the data frame or matrix. Another possibility is to click on the button `data`.

*If you like you can try it out. You can solve this task optionally. It is not necessary for the following tasks.*

```{r "1__5",optional=TRUE, results='asis'}
#< task
# enter your command here
#>
head(dat)
```

<br>You can see that there are $17$ different columns. Most of the column names are self-explanatory. But now we want to go more into detail. We divide the different variables of the columns of our data frame into four groups:
<br>- **electricity consumption** (`elec`) and **natural gas consumption** (`gas`),
<br>- **effective year built** (`EYB`),
<br>- several **housing characteristics** like square footage (`sqfeet`), the number of bathrooms (`baths`), the number of bedrooms (`beds`) and central air-conditioning (`centralair`),
<br>- **weather variables** which are called average heating degree days (`HDD`) and average cooling degree days (`CDD`).

We will cover the different groups in separate exercises. Exercise 1.1 contains the first two groups, namely the variables representing the energy consumption and second the variable effective year built. Exercise 1.2 deals with the housing characteristics and in exercise 1.3 we will analyse the weather variables.

*Exercise 1 refers to the pages 5 - 10 and 16 of the paper and to table 1.*

## Exercise 1.1 -- Energy consumption and effective year built

This exercise deals with three really important variables of our data frame. First we will take a closer look at the two variables `elec` and `gas` which represent the energy consumption of the residences and then we will analyse the meaning of the variable `EYB` which stands for effective year built. Last we will use these variables to plot several histograms. But first of all we have to load the data.    

*Because you just started a new exercise you now have to click on the `edit` button to be able to solve the next task.*

**Task:** Load the data set `dat_energy.dta`. It contains only the important variables we need for this exercise. Don't forget to load the required package which is called `foreign`. Press `check` afterwards.   
```{r "1_1"}
#< task
# load the package
# enter your code here...
#>
library(foreign)
#< hint
display("Use the command library() and type the name of the package inside.")
#>
```

```{r "1_1__2"}
#< task
# read in the data set dat_energy.dta and assign it to the name dat
# enter your code here...
#>
dat = read.dta("dat_energy.dta")

#< hint
display("Your command should have the following form: dat = read.dta(\"???\"). Replace the ??? by the name of the data set.")
#>
```

### Electricity and natural gas consumption

The two variables `elec` and `gas` are the most important ones because they allow us to compare the energy efficiency of residences built before and after the energy code change. We have two variables instead of only one because electricity is mainly used for air-conditioning and natural gas for heating.
<br> To make the connection between the building energy code and the actual energy consumption more clear you have to know that the energy code establishes a minimal norm for energy efficiency in the areas of space heating, space cooling and water heating. Therefore heating and air-conditioning which is used for cooling are the "two main end-uses that are targeted by the energy codes" (paper, p. 4).

Now take a closer look at both variables.

#< info "useful mathematical functions"

Here is a list of some useful mathematical R functions.  
```{r "1_1__3"}
min(c(2,8,4,7)) # calculates the mimumum of the given vector

x = c(2,8,4,7)
x
max(x) # calculates the maximum of the vector x
range(x) # calculates the minimum and maximum of x and returns a vector with both values 
mean(x) # returns the mean of the elements of the vector x
sd(x) # returns the standard deviation of x
```
#>

**Task:** Choose the column `elec` of the data frame `dat`. Use the \$-operator and write `dat$elec` to select this column. Then compute the minimum and the maximum of the electricity consumption with the command `range()`.
```{r "1_1__4"}
#< task
# enter your code here...
#>
range(dat$elec)
#< hint
display("Put the name of the selected coloum in the brackets of the command range(). But without quotation marks. You get some additional advice in the above info box.")
#>
```

The result is a vector which states two values. First the minimum and then the maximum of the monthly electricity consumption in kWh of all our data.
<br> It is also possible to compute the minimum and the maximum separately.

**Task:** First compute the maximum of the natural gas consumption and then the minimum. Similar like the task before use the \$-operator. But now you have to select the column `gas`.
```{r "1_1__5"}
#< task
# enter your code here...
#>
max(dat$gas)
min(dat$gas)
#< hint
display("Please compute the maximum first. Use the commands max() and min() and take a look at the task above or the info box. It works similar like with the function range().")
#>
```

The two values represent the largest and smallest natural gas consumption of one household compared to all other households in our data set. The gas consumption is measured per month and in therms.

#< info "therm"
**Therms** are a measure for heat energy and in the U.S. it is used to measure the natural gas consumption of residences. 
<br> 1 **therm** is equal to 100,000 British Thermal Units (BTU). 
<br> And 1 BTU "is the heat required to raise the temperature of one pound of water by one degree Fahrenheit" (https://www.eia.gov/tools/faqs/faq.cfm?id=45&t=8).
<br> You can also convert therms into kWh: 1 **therm** corresponds roughly to 29.3 kWh.

*references: http://mapawatt.com/2010/02/17/what-therm, https://www.eia.gov/tools/faqs/faq.cfm?id=45&t=8 and* http://www.convertunits.com/info/therm+[U.S.]
#> 

When you compare the results for the electricity and gas consumption you can see that the maximal consumption of electricity is a lot higher than for natural gas. A reason for this is the climate in Gainesville or more general in Florida. The climate situation can be described as following: there are much more days with high temperature than with low temperature where you need gas for heating. Therefore, households in Florida typically use more electric energy for cooling by the use of air-conditioning than gas for heating. In exercise 1.3 you will get evidence for it. 
Now we will turn to another important variable. 

### Effective year built (EYB)

The variable effective year built or short `EYB` is correctly spoken a housing characteristic but it is of primary interest so we consider it separately. `EYB` allows us to determine whether a residence was built under the regulations of the old energy code or under the new one, established in the year $2002$. More exactly the variable `EYB` declares the year in which the construction of a house has been completed. This means the year in which the final inspection took place and not the year of some extensive remodeling(according to paper, p. 7).
<br> With the help of the variable `EYB` we can now differ between pre-code and post-code change residences and divide our data set in these two groups. Pre-code change residences are houses with an `EYB` of **2001 and earlier** and we define residences as post-code change if their `EYB` are the years of **2003 and later**. We dropped houses with an `EYB` of $2002$ because it is the year of the code change and therefore not possible to uniquely define the energy code of the corresponding residence.

#< info "filter()"

The function `filter()` is contained in the package `dplyr`. The command returns all rows of a data frame that fulfill a certain condition.  
First specify the data frame and then state the condition. Separate both by a comma.
We use the data frame `dat`. We want to generate a subset of this data frame containing only the billing data of the year 2002. (The variable is called `year`.) The resulting data frame should be called `dat_02`. 

```{r "1_1__6",eval=FALSE}
# load the package
library(dplyr)

# use the function filter
dat_02 = filter(dat, year == 2002)
```

Instead of $==$ you can for example use $<$ or $>=$ to generate different subsets of the data frame.  

If you want to have more information about this function you can look at https://www.rdocumentation.org/packages/dplyr/versions/0.5.0/topics/filter.
#>

**Task:** Divide the data set `dat` into two parts. One should include all pre-code residences and the other all residences built after the code change. Use the variable `EYB` for separation in the two groups. Assign the new data sets with the names `pre_code` and `post_code`. The function `filter()` from the package `dplyr` will simplify your work.
```{r "1_1__7"}
#< task
# load the package
library(dplyr)

# data frame of all residences built before the code change in the year 2002
# enter your code here...
#>
pre_code = filter(dat, EYB <= 2001)

#< hint
display("Your commands should have the following form: pre_code = filter(???,??? <= ???). To state the condition use the variable EYB and the correct year. If you need further information how to use the function filter(). Check the info box above.")
#>

#< task
# data frame of all post-code change residences (built in the year 2003 or later)
# enter your code here...
#>
post_code = filter(dat, EYB >= 2003)
#< hint
display("Your commands should have the following form: post_code = filter(???,??? >= ???). To state the condition use the variable EYB and the correct year. If you need further information how to use the function filter(). Check the info box above.")
#>
```

#< award "dplyr user level 1"
Great! You are able to use a function of the dplyr package. This package is really helpful for doing data preparation. So keep in mind! It will really simplify your work.
<br> While going on with the problem set you will get to know some other functions of this package.
#>

*This task can be solved optionally.*

**Task:** You can test your results of the above task for correctness. One possibility is to count the number of residences in the data set `dat` and then do the same for both smaller data sets. The first number should be the same as the sum of the latter ones.
<br> The commands are already given. The function `length()` counts the number of observations of the vector which is written in the brackets. The command `unique()` removes all duplicate elements. We need this function in our data frame because we have a lot of billing data for each individual residence and we only want to count each household once. Click on the button `check` to see if the results are correct.
```{r "1_1__8",optional=TRUE}
#< task
# total number of residences 
total_number = length(unique(dat$home_id))
total_number

# First compute the number of residences built before the code change. 
pre_number=length(unique(pre_code$home_id))
pre_number

# Second compute the number of post-code change residendes.
post_number=length(unique(post_code$home_id))
post_number

# Third calculate the sum of pre_number and post_number.
sum(pre_number,post_number)
#>
```

When your sum corresponds to the total number of $2239$ residences you have split the data frame correctly into the two groups. The results also show that the group of pre-code residences is bigger than the group of residences built after the code change. The pre-code data frame contains $1293$ households and in the post-code group are $946$ different residences. 

We use the two data frames `pre_code` and `post_code` for a visualisation of our data. We want to compare the energy consumption of the before and after code change residences. We also use the two data frames in exercise 2.1 and 2.2. 

### Visualise the data -- energy consumption of pre- and post-code residences

Our main aim is to detect if the building energy codes are useful in saving energy. Therefore we want to compare the energy consumption of the pre-code residences and of the post-code buildings. But we also want to know a little bit more about the energy consumption of the individual households. 
<br> First we will plot the electricity consumption and then the natural gas consumption. In each section we will draw three plots: first a histogram of the energy consumption of the pre-code residences, second a histogram of the consumed energy of post-code buildings and third a graph with both histograms. A histogram graphically represents the frequency distribution of the data and so we can get a rough idea of the distribution of our data frame (according to Maindonald and Braun (2007), p. 44 ff.).   
Because the pre-code and post-code groups are of different size we have to normalise the counts of the histogram to be able to compare both plots. 

#### a) Electricity consumption

**Task:** Draw a histogram of the electricity consumption of the residences built before the code change.
To get a nice output we use the function `ggplot()` of the package `ggplot2`. The function `ggplot()` creates the basic level of graphic. The final plot is constructed incrementally by using the $+$ operator to add additional elements like histograms or lines or the title of the axes. If you haven't used `ggplot()` before it is a little bit tricky so you don't have to enter the commands by yourself. But take a look at the given code and try to understand the commands.

I will also explain the code to you. Before your can start with plotting you have to load the package `ggplot2`. Then in the first step we create the basic level with the command `ggplot()`. In the brackets you have to specify the data (`pre_code`) and the axis with the command `aes()`. The x-axis should cover the electricity consumption so we need the variable `elec`. We also add `xlim()` to determine the range of the x-axis. We assign it to the variable p. The second step is used to draw the histogram and add it to the basic level. Therefore we write `p +` and then use the command `geom_histogram()` to create the histogram. To get relative frequencies instead of absolute ones we write `aes(y=..density..)`. Then we have to specify the width of the bins (`binwidth=...`). We also change the colour of the bins (`fill="..."`) and the colour of the borders of the bins (`colour="..."`). Then we additionally add the title of the x-axis with the command `xlab("...")`. We save all previous changes in the variable `p1`. Last we call the variable `p1` to plot it. 
<br>*references: https://cran.r-project.org/web/packages/ggplot2/ggplot2.pdf, https://www.rstudio.com/wp-content/uploads/2015/06/ggplot2-german.pdf, http://docs.ggplot2.org/0.9.3.1/geom_histogram.html, http://docs.ggplot2.org/0.9.3/xylim.html, http://stackoverflow.com/questions/29287614/r-normalize-then-plot-two-histograms-together-in-r*

*If you havn't solved the last task you now have to click on the `edit` button to be able to solve the next task.*

Just press `check` to show the plot.
```{r "1_1__9",fig.height=5, fig.width=8, warning=FALSE}
#< task
# load the package
library(ggplot2)

# 1. Create the basic level and determine the range of the x axis. Assign it to the
# variable p.
p = ggplot(data=pre_code, aes(elec)) + xlim(0,7000)
 
# 2. Add the histogram and the title of the x-axis. Assign it to the variable p1.
p1 = p + geom_histogram(aes(y=..density..), binwidth=100, colour="black", fill="blue") + 
  xlab("electricity consumption of pre-code residences") 

# 3. Call the variable to plot it.
p1  
#>
```
<br>Now we can see how the data is located. The major part of the households' monthly electricity consumption lies between $100$ kWh and $3000$ kWh. The greatest amount of residences consume between $500$ and $600$ kWh of electricity per month. The data is not symmetric but positively skewed because it has a long tail to the right. Hence, our data can't be normally distributed (according to Maindonald and Braun (2007), p. 58). Further details about interpreting histograms you can read on http://www.itl.nist.gov/div898/handbook/eda/section3/histogra.htm.

**Task:** Now we want to draw a histogram of the electricity consumption of the post-code residences. 
The structure of the commands is the same as in the previous task but we have to make some changes. First we have to change the data frame and then we have to adapt the title of the x-axis. We additionally change the colour of the histogram from blue to red. Press `check`. 
```{r "1_1__10",fig.height=5, fig.width=8, warning=FALSE}
#< task
# 1. Create the basic level and determine the range of the x axis. Use the data frame
# post_code.
q = ggplot(data=post_code, aes(elec)) + xlim(0,7000)

# 2. Add the histogram and the title of the x-axis. Change the colour of the bins and the 
# title of the x-axis.
q1 = q + geom_histogram(aes(y=..density..), binwidth=100, colour="black", fill="red") + 
  xlab("electricity consumption of post-code residences")
 
# 3. Plot it.
q1 
#>
```
<br>The histogram of the electricity consumption of post-code residences looks really similar to the one before. But the small values of the energy consumption (roughly between $600$ and $1900$) are even more frequent in this histogram. Another difference is that there is not only one peak but two. The greatest amount of post-code residences use between $600$ and $700$ kWh of electricity per month. The second and a little bit smaller peak includes a electricity consumption of $900$ to $1000$ kWh per month.

**Task:** To compare both histograms it would be better to have one graph with one histogram below the other. To get such an output we will use the function `grid.arrange()` out of the package `gridExtra`. Further information you get on the following page:  https://cran.r-project.org/web/packages/gridExtra/gridExtra.pdf.

Don't forget at first to load the package with the command `library()`. The histogram of the pre-code residences was called `p1` and the one of the post-code buildings `q1`. Click on the button `check` to show the graph.
```{r "1_1__11",fig.height=8, fig.width=8, warning=FALSE}
#< task
# load the package
library(gridExtra)
 
# plot it 
grid.arrange(p1, q1) 
#>
```
<br>Now it is easier to compare both histograms. We already mentioned that the smaller values of the average electricity consumption of the post-code residences are more frequent. Further we can see that the outliers which are greater than $4000$ kWh per month are less frequent in the group of residences built after the code change. These two differences give us a first idea of the success of building codes on the reduction of electricity consumption. We will prove this result with the help of different statistical analyses in the following exercises (especially exercise 3 and 4).

#### b) Natural gas consumption

Now we consider the natural gas consumption of the Gainesville residences.

**Task:** Plot a histogram of the natural gas consumption of the pre-code residences. Replace the ??? in the code with the correct commands and afterwards uncomment these lines. Then press `check`.   
```{r "1_1__12",fig.height=4, fig.width=8, warning=FALSE}
#< task
# 1. Create the basic level and determine the range of the x-axis and of the y-axis. Use 
# the data frame pre_code.
# g = ???(data=???, aes(gas)) + xlim(0,150) + ylim(0,0.05)
#>
g = ggplot(data=pre_code, aes(gas)) + xlim(0,150) + ylim(0,0.045)

#< task 
# 2. Add the histogram and the title of the x-axis. Assign it to the variable g1. 
# g1 = g + ???(aes(y=..density..), binwidth=2, colour="black", fill="blue") + 
#   ???("natural gas consumption of pre-code residences")
#>
g1 = g + geom_histogram(aes(y=..density..), binwidth=2, colour="black", fill="blue") + 
  xlab("natural gas consumption of pre-code residences")
  
#< task  
# 3. Plot it.
# ???
#>
g1
```

We will interpret this histogram in comparison with second histogram at the end of this exercise. 
**Task:** Now plot a histogram of the natural gas consumption of the post-code residences. The colour of the bins should be red. Replace the ??? in the code with the correct commands and then uncomment these lines. 
```{r "1_1__13",fig.height=4, fig.width=8, warning=FALSE}
#< task
# 1. Create the basic level and determine the range of the x-axis and of the y-axis.
# h = ggplot(data=???, ???) + xlim(0,150) + ylim(0,0.05)
#>
h = ggplot(data=post_code, aes(gas)) + xlim(0,150) + ylim(0,0.045)

#< task 
# 2. Add the histogram and the title of the x-axis. The colour of the bins should be red.
# h1 = ??? + geom_histogram(aes(y=..density..), binwidth=2, colour="black", ???) + 
#   xlab("natural gas consumption of post-code residences") 
#>
h1 = h + geom_histogram(aes(y=..density..), binwidth=2, colour="black", fill="red") + 
  xlab("natural gas consumption of post-code residences") 

#< task  
# 3. Plot it.
### ???
#>
h1

```

**Task:** Plot both histograms in one graph. Use the function `grid.arrange()`. 
```{r "1_1__14",fig.height=8, fig.width=8, warning=FALSE}
#< task
# enter your code here...
#>
grid.arrange(g1, h1) 

#< hint
display("Your command should have the following form: grid.arrange(histogram1,histogram2).")
#>
```

#< award "plot histograms with ggplot2"
Great! You have drawn nice histograms with ggplot2. 
<br> Visualisation of the data is always helpful to get more familiar with it.
#>

Now let us interpret and compare both histograms. We can see how the data for the monthly natural gas consumption is distributed. The major part of the households' gas consumption lies between $0$ and $25$ therms. The greatest amount of residences consume between $6$ and $8$ therms of gas per month. The data is not symmetric but positively skewed with a long tail to the right. Hence, the data is not normally distributed (reference: Maindonald and Braun (2007), p. 58). When we compare both histograms we can see that the gas consumption of post-code residences between $16$ to $24$ therms is more frequent and the positive outliers are less frequent than for pre-code buildings. But the differences are not that visibly than for the electricity consumption.

*nicer histogram with more features as extra R file - which one should I use?*

*Exercise 1.1 refers to the pages 5 to 10 of the paper and to table 1.*

## Exercise 1.2 -- Housing characteristics

In the previous exercise we have taken a closer look at the first two groups of the variables of our original `billingdata.dta` data frame. This exercise deals with the third group of important variables, i.e. with the housing characteristics.
<br> First we will consider the characteristics of one arbitrary selected residence. Then we will plot some graphs of the housing attributes of all residences in our data frame. 
<br> But first of all we have to load the necessary data set.    

*You have started a new exercise so you have to click on the `edit` button to be able to solve the next task.*

**Task:** Load the data frame `dat_hc.dta` which only contains information about the characteristics of the residences. Therefore, this data set only has 9 different columns. Just press `check`. If you like to take a look at the data frame press `data`. 
```{r "1_2"}
#< task
# load the package
library(foreign)

# read data set dat_hc.dta into R and assign it to dat_hc
dat_hc = read.dta("dat_hc.dta")
#>
```

Let's show the first few rows of the data frame `dat_hc` by clicking on the `check` button. 
```{r "1_2__2"}
#< task
head(dat_hc)
#>
```

The different columns of the data frame `dat_hc` represent the different characteristics of a residence. Each column stands for one variable. In our data set we have information about the following attributes: zip code (`zip`), square footage (`sqfeet`), the number of bathrooms (`baths`), the number of bedrooms (`beds`), the number of stories (`stories`), central air-conditioning (`centralair`) and the roof type (`shingled`).
<br> Most of the variable names are self explaining. The last two characteristics `centrailair` and `shingled` are **indicator variables** and only attain the values $1$ and $0$. Meaning "yes" and "no" respectively (according to Stock and Watson (2007), p. 158). 
<br>In more detail, if $`centralair`== 0$ the residence does **not** have installed a central air-conditioning. For the variable `shingled` which describes a special roof type it works the same: $`shingled`== 0$ means that the house does **not** have a singled roof. 

We need these different variables in later exercises (exercise 3 and 4) as explanatory variables in our regression models.

But first I will show you how to select one housing characteristic of the data set. Let us choose an arbitrary household. We want to take the residence with the identification number (`home_id`) $128196$. To choose this household we use the function `filter()` out of `dplyr` package as we already did in some previous tasks. Then we save the result in the data frame `example_home`. 
<br> We are interested in the number of bathrooms and the number of bedrooms of this particular residence. We will use the function `select()` of the `dplyr` package to choose specific columns of the data frame. The command `select(dat,col1,col2)` keeps the two columns with the names `col1` and `col2` of the data frame `dat`. In our case the two columns `baths` and `beds` of the data frame `example_home` are selected. Press `check` to solve the following task.
<br> If you want further information about the function `select()` you can take a look at https://cran.r-project.org/web/packages/dplyr/dplyr.pdf.

```{r "1_2__3"}
#< task
 library(dplyr)
# generate a new data frame with information only about the residence with home_id 128196
example_home = filter(dat_hc, home_id == 128196)

# determine the number of bathromms and bedrooms
select(example_home,baths,beds)
#>
```

In the first column of resulting table you can see the number of bathrooms and in the second the number of bedrooms of the residence with the identification number $128196$. 

**Task:** We want to get more information about this house. So we choose some different characteristics namely zip code `zip`, square footage `sqfeet` and the number of stories `stories`. You should answer the questions of the quiz below. You can use the code box to enter some command that will help you to find the correct answers.
```{r "1_2__4",optional=TRUE}
#< task
# enter your code here...
#>

#< hint
display("The following command will produce all necessary results select(example_home,zip,sqfeet,stories).")
#>
```

#< quiz "residence No. 128196"
question:
parts:
  - question: 1. What's the zip code of this residence?
    answer: 32606
    roundto: 1

  - question: 2. How large is this residence? (size in square feet)
    answer: 2602
    roundto: 1

  - question: 3. How many stories does this building have?
    answer: 1
    roundto: 1

success: Great, all questions correctly solved!
failure: Not all questions solved correctly. Try again.
#>

<br>*Is it possible to create a map, when we only have the information of the zip code?*

<br> You got to know more information about the characteristics of one arbitrary choosen residences. Now we expand our analysis on the entire data frame `dat_hc`. We will focus on the variables describing the size of the houses. There are two different columns in the data set: `sqfeet` and `logfeet`. The variable `logfeet` is the natural logarithm of the variable `sqfeet` which measures the size of a residence in square feet. We afterwards need the variable `logfeet` in our regression analyses in exercise 3.  
<br> Now we want to modify the data frame and add a new column `sqmetre`. This variable also measures the size of a house but in square metre and not in square feet. A reason for this modification is that square metre is the common unit in Germany and therefore we have a better understanding of its proportion.    

**Task:** Add a new column `sqmetre` to the data frame `dat_hc`. Use the function `mutate()` out of the `dplyr` package. Further information about this function you get on  https://cran.r-project.org/web/packages/dplyr/dplyr.pdf. Assign the expanded data frame with the name `dat_sqm`. 
<br> We can transform the variable `sqfeet` into the variable `sqmetre` by multiplying `sqfeet` with $0.092903$ (reference: http://www.asknumbers.com/square-feet-to-square-meter.aspx). We additionally round the numbers to two decimal places. Press `check` to perform the calculation.
```{r "1_2__5"}
#< task
dat_sqm = mutate(dat_hc, sqmetre = round(sqfeet*0.092903, digits=2))
#>
```

Now let's take a look at these three variables. To better compare them we only select the important columns of the data frame `dat_sqm` and save the reduced data frame as `dat_size`.  
```{r "1_2__6"}
#< task
dat_size = select(dat_sqm, home_id, sqfeet, sqmetre, logfeet)
head(dat_size)
#>
```

<br>**Task:** Calculate the size of the smallest and of the largest residence in square metre. Use the data frame `dat_size`. One command is enough here. 
```{r "1_2__7"}
#< task
# enter your code here...
#>
range(dat_size$sqmetre)
#< hint
display("Your command should have the following form: range(dat_name$column_name). Insert the correct data frame and column.")
#>
```
You see that the smallest residence is about $73$ square metre and the largest one is roughly $9.5$ times bigger.

### Visualise the data -- housing characteristics

Now we want to draw some plots to illustrate the different characteristics and thus get a much deeper knowledge of these variables. We consider the entire data frame `dat_hc` and not only one residence like in the previous task.  
<br> At the end of this section you should answer some questions regarding the housing characteristics. The following graphs will provide the answers. Hence, if you draw the plots you should be able to solve the quiz correctly.

First we want to plot the different number of bedrooms against their frequencies.

**Task:** Draw a bar graph of the column `beds` of the data frame `dat_hc`. Use the functions `ggplot()` and `geom_bar()` of the package `ggplot2`. Further information about these two functions you can get on the following two pages https://www.rstudio.com/wp-content/uploads/2015/06/ggplot2-german.pdf and http://docs.ggplot2.org/0.9.3.1/geom_bar.html.

Press `check` to draw the graph.
```{r "1_2__8",fig.height=3, fig.width=4}
#< task
# load the package
library(ggplot2)

# Create the basic level by using the data frame dat_hc. The variable beds should be 
# plotted on the x-axis. 
p = ggplot(dat_hc, aes(beds))

# Add the bar graph and the title of the x-axis. 
p1 = p + geom_bar(fill= "green", colour="black") + xlab("number of bedrooms")

# Plot it.
p1
#>
```

**Task:** Now we want to plot a bar graph of the number of bathrooms. Replace the ??? in the code with the correct commands and then uncomment these lines. 
```{r "1_2__9",fig.height=3, fig.width=4}
#< task
# Create the basic level by using the data frame dat_hc. The variable baths should be 
# plotted on the x-axis. 
#q = ggplot(???, aes(???))
#>
q = ggplot(dat_hc, aes(baths))

#< task
# Add the bar graph and the title of the x-axis. 
#q1 = q + ???(fill= "blue", colour="black") + ???("number of bathrooms")
#>
q1 = q + geom_bar(fill= "blue", colour="black") + xlab("number of bathrooms")

#< task
# Plot it.
# ???
#>
q1
```

**Task**: Draw a bar graph of the number of residences with central air-conditioning. The variable is called `centralair` and takes the values 0 (no air-conditioning) and 1 (integrated central air-conditioning). Enter your commands and press `check` afterwards. Click on the `hint` button if you need further advice. 
```{r "1_2__10",fig.height=3, fig.width=4}
#< task
# First create the basic level by using the data frame dat_hc. The variable centralair 
# should be plotted on the x-axis. Assign it to the name r. 
# enter your code here...
#>
r = ggplot(dat_hc, aes(centralair))
#< hint
display("Your first command should look like: r = ggplot(data_name, aes(var_name)).")
#>

#< task
# Then add the bar graph and the title "central air conditioning" to the x-axis . The 
# bars of the graph should be red and the borders black. Assign it to r1. 
# enter your code here...
#>
r1 = r + geom_bar(fill= "red", colour="black") + xlab("central air conditioning")
#< hint
display("Your second command should have the following form: 
        r1 = r + geom_bar(fill= \"???\", colour=\"???\") + xlab(\"title\").")
#>

#< task
# Last plot it.
# enter your code here...
#>
r1
#< hint
display("Write the name of the variable to plot it.")
#>
```

#< award "plot bar graphs with ggplot2"
Great! You can use another function of the ggplot2 package to visualise the data.

#>

#< quiz "housing characteristics"
question:
parts:
  - question: 1. What's the most frequent number of bedrooms?
    answer: 3
    roundto: 1

  - question: 2. What is the maximal number of bathrooms of a residence?
    answer: 6
    roundto: 1
    
  - question: 3. Most of the residences have installed a central air-conditioning. Is this statement correct? Answer with $yes$ or $no$. 
    answer: yes
    
success: Great, all questions correctly solved!
failure: Not all questions solved correctly. Try again.
#>

You find that central air-conditioning is really important for households in Gainesville, especially in the summer months. Let us now look closely at the relationship of residential energy consumption and central air-conditioning. Generally, air-conditioning is the main end use of residential electricity consumption. To be more precise, the housholds which are located in the south atlantic region of the U.S. use $21.2\%$ of the electricity for air-conditioning (according to U.S. Energy Information Administration (2009), link: http://www.eia.gov/consumption/residential/data/2009/index.php#fueluses).  
<br> In contrast to this, the main end uses of residential natural gas consumption are space and water heating. In numbers, south atlantic residences need $6.0\%$ and $5.5\%$ of the natural gas consumption for space heating and water heating, respectively (refer to U.S. Energy Information Administration (2009), link: http://www.eia.gov/consumption/residential/data/2009/index.php#fueluses). 
<br> Keep these relationships in mind, we need them later in  exercise 3 and 4.

To sum up, this exercise dealt with the variables which describe the characteristics of the residences. In the following exercise we will turn to the weather variables. 

*Exercise 1.2 refers to the pages 7 and 10 of the paper and to table 1*

## Exercise 1.3 -- Weather variables

This exercise will provide detailed information about the two weather variables: the average heating degree days (**AHDD**) and the average cooling degree days, short **ACDD**. To consider these variables in more detail is important because space heating and cooling are strongly correlated with the weather and temperature outside the residence. Therefore the authors of the paper collected weather data. They used the data from the National Climatic Data Center of one weather station which was located at the airport of Gainesville.  
<!-- visualization of the location of the weather station (no. 083362 in the National Weather Service's Cooperative Station Network) in comparison to Gainesville? -->
In a second step they merged these data with the monthly billing data. 
<br> These two steps are only preparation and therefore not important for our analysis. But if you like detailed information you may take a look at page 9 of the paper.

Finally, the authors calculated the two weather variables average heating degree days (AHDD) and the average cooling degree days (ACDD). These two variables are provided in the data frame with the column names `HDD` (representing AHDD) and `CDD` (standing for ACDD). In the **info box** below you get some information about the calculation of these variables. 

#< info "Calculation of AHDD and ACDD"
We want to calculate the two variables AHDD and ACDD from temperature data. Several steps are necessary:
<br> First we declare a reference point. The standard one is **65 degree Fahrenheit (F)** which refers to 18.3 degree Celsius. 
<br> Second we have to calculate the average daily temperature.
<br> Then we can determine the AHDD and ACDD by considering the difference of the reference point and the average temperature: 
 - If the average temperature is below the reference point we can calculate the **AHDD** by subtracting the average temperature from the reference temperature. The resulting value is the number of heating degrees per day. 
 As formula: $AHDD = 65^\circ~F - \textrm{average}~ \textrm{temperature}$.
 - On the opposite, if the average temperature is above 65 degree F the **ACDD** can be calculated by subtracting the reference point from the average temperature. The resulting value is the number of cooling per day. 
 As formula: $ACDD = ~\textrm{average}~ ~\textrm{temperature} - 65^\circ~F$.
 
*reference: p. 9 of the paper and EPA (2014), https://www3.epa.gov/climatechange/pdfs/print_heating-cooling-2014.pdf*
#>

Before we can start our analysis we have to load the data. First click `edit` to be able to start with the first task. Then press `check`. All comments are already given because you loaded a data frame several times before.
<br> We load the initial data frame `billingdata.dta` which we already used in exercise 1. Afterwards we perform some computations to prepare the data frame. 
```{r "1_3"}
#< task
# load the package
library(foreign)

# read data set billingdata.dta into R and assign it to dat
dat = read.dta("billingdata.dta")
#>
```
To get some basic information about the weather variables let us compute some summary statistics. Therefore we need some functions out of the `dplyr` package. You already got to know the `select()` command. But there is another useful function called `summarise()` which summarises multiple values (in our case a whole column) to one value. You can get further information about this function on the following page: https://cran.r-project.org/web/packages/dplyr/dplyr.pdf.

In the following task we use both commands one after the other. First to select only the columns `CDD` and `HDD` from the data set `dat` and then to calculate the maximum of each column. We want to use the ${\%>\%}$ - operator to combine both commands in an easily readable structure. You find more information about this operator in the **info box** below.

#< info "Pipe-operator"
Here are some further information how to use the ${\%>\%}$ - operator in combination with `dplyr` functions. The ${\%>\%}$ - operator is provided by the `magrittr` package and it will be imported by `dplyr` without having additionally loaded `magrittr`.    
<br> The idea of this operator is to avoid nested functions which you have to read from the insight to the outside but allow to read the functions from the left to the right. This works because the output of function on the left hand side is used as input of the function on the right hand side. Therefore you don't have to specify the data frame inside the functions, e.g. don't write `select(dat_name, col_names)` but `dat_name %>% select(col_names)`.    
<br> A simple rule for writing: first specify the data frame and then write the functions one after the other. Connect each function with the ${\%>\%}$ - operator. To get a nice structure write only one command and the ${\%>\%}$ - operator per code line and then start a new line. This helps to make your code more readable and better understandable.      

*references: http://genomicsclass.github.io/book/pages/dplyr_tutorial.html and http://blog.revolutionanalytics.com/2014/07/magrittr-simplifying-r-code-with-pipes.html*
#>

Press `check` to see the solution.
```{r "1_3__2"}
#< task
# load the package
library(dplyr)

# use the %>%-operator to combine the functions
dat %>%
  select(CDD,HDD) %>%
  summarise(max(CDD),max(HDD))
#>
```

<br>The values indicate the highest difference of the average daily temperature and the reference temperature. In the first case of exceeding $65^\circ~F$ by roughly $17.6^\circ~F$. And in the latter case of falling below the reference point by $11.8^\circ~F$. The results also show that the maximum of the ACDD is higher than the maximum of the AHDD. The reason for this result is that Florida is a state with mild and sunny climate. For example, the statewide annual average  temperature of Florida in the year $2002$ was $71.4^\circ~F$ with the lowest monthly average temperature of $57.5^\circ~F$ in December (according to http://climatecenter.fsu.edu/products-services/data/statewide-averages/temperature).
<br>Further climate data of Gainesville you can find on http://www.usclimatedata.com/climate/gainesville/florida/united-states/usfl0163/2016/1.

**Task:** Select the columns `CDD` and `HDD` of the data frame `dat` and then compute the mean of the average cooling degree days (`CDD`) and the average heating degree days (`HDD`). Use the ${\%>\%}$ - operator.
```{r "1_3__3"}
#< task
# enter your code here...
#>
dat %>%
  select(CDD,HDD) %>%
  summarise(mean(CDD),mean(HDD))

#< hint
display("Your command should have the same structure like in the code box above. You only have to use another function to calculate the mean. You also may take a look at the first info box in exercise 1.1.")
#>
```

#< award "dplyr user level 2"
Great! You can easily deal with the functions out of the dplyr package and with the ${\%>\%}$ - operator.
You are now able to handle a data frame with a lot of data and produce summary statistics.
#>

The table shows that also the average of the ACDD is higher than the mean value of the AHDD. Which can also be explained by the the climate situation in Florida.  

### Visualise the data -- weather variables

In the previous tasks you got some overview of the weather variables. Now we want to visualise both weather variables AHDD and ACDD to see how they behave over several years. More precisely we want to plot the average cooling degree days and the average heating degree days by month and year over the time interval from $2004$ to $2006$.

Our first step is to prepare the data frame. We already did similar preparations in the previous task. But now we need the columns `CDD`, `HDD`, `month` and `year` of the data set `dat`. We use the command `select()` to keep them. 
<br> Second, we need the `group_by()` function. This function pools the rows of a data frame together which take the same values within a specified variable or group. 
<br> Last, we use the command `summarise()` to calculate the mean value of the variables `CDD` and `HDD` and additionally we assign the resulting values to `ACDD` and `AHDD`. 
<br> So at the end we get the monthly average of the weather variables over a three year period from the years $2004$ to $2006$. In sum there are $36$ mean values for each of the two variables. 
<br>**Task:** You only have to press `check` to create a new data frame `dat_weather` and show the result. We use this data frame in the following task to plot the weather variables by month and year. If you want to see the whole data frame of `dat_weather` press the button `data`.
```{r "1_3__4"}
#< task
# create a subset of the data frame dat and assign it to dat_weather
dat_weather = dat %>%
  # keep the four columns CDD, HDD, month and year
  select(month,year,CDD,HDD) %>%
  # group the data with the same month and year together
  group_by(year,month) %>%
  # calculate the mean value of the columns CDD and HDD
  summarise(ACDD = mean(CDD), AHDD = mean(HDD))

# show a part of the data frame
head(dat_weather) 
#>
```
The resulting data frame `dat_weather` contains important values. We will use the values of the columns `ACDD` and `AHDD` directly for plotting. They will be drawn on the y-axis.   

But the values which we want to plot on the x-axis are still missing. In our graph the x-axis will represent the time. Therefore we have to do a further step of preparations. 
<br>The aim is to add a new column to the data frame `dat_weather` which contains our time data. More precisely, the new column will be called `date` and it will cover the monthly dates from January $2004$ to December $2006$. We use the function `mutate()` to add this new column. But before we can do this we have to remove the existing grouping of the data with the command `ungroup()`. More information about this function you can find on  https://cran.r-project.org/web/packages/dplyr/dplyr.pdf. 
<br>We create the time sequence with the command `seq()`. We have to define the initial date (`from=...`) and the end date (`to=...`) and the time intervals between (`by=...`) these two dates. More details you can read on https://stat.ethz.ch/R-manual/R-devel/library/base/html/seq.Date.html. If you like to know more about the different date formats in R you can take a look at http://www.statmethods.net/input/dates.html. 

The last step of preparation is to save the expanded data frame as `plot_weather` to be able to use it for plotting the weather variables.   
```{r "1_3__5"}
#< task
# add a date column to the data frame dat_weather
plot_weather = dat_weather %>%
  # cancel the grouping operation
  ungroup() %>%
  # create a time sequence and save it as new column with the name date
  mutate(date=seq(from=as.Date("2004-01-01"), to=as.Date("2006-12-01"), by="month"))

# show a part of the data frame
head(plot_weather)
#>
```

<br>Now we can use the data frame `plot_weather` to plot the average of the weather variables on the y-axis against the monthly time period from $2004$ to $2006$ on the x-axis. We want to use the function `ggplot()` and `geom_line()` of the package `ggplot2` to get a nice line graph. The code looks more complicate than the ones before. Don't worry. You not have to understand all comments in detail. The functions are used to get a nice plot with different coloured lines, with a legend and with an adapted description of the x-axis. 

*references: https://www.rstudio.com/wp-content/uploads/2015/06/ggplot2-german.pdf, http://docs.ggplot2.org/0.9.3.1/scale_date.html, http://www.statmethods.net/input/dates.html,  http://www.cookbook-r.com/Graphs/Legends_(ggplot2)/, http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/#change-size-of-tick-text-axis.text.x*
<!-- reference missing: two lines in different colours (add manually and also change legend) -->

Press `check` to create the plot. 
```{r "1_3__6",fig.height=6, fig.width=9}
#< task
# load the package ggplot2
library(ggplot2)
# load the package scales which is necessary for the second step
library(scales)

# 1. Create the basic level and add the titels of the y-axis and the x-axis. Assign it to 
# the variable w. 
w = ggplot(plot_weather, aes(x=date)) + ylab("Degree Days") + xlab("Time")

# 2. Add the lines representing the variables ACDD and AHDD. Use different colours. 
# Additionally adapt the description of the x-axis.
w1 = w + geom_line(aes(y=ACDD, col="ACDD")) + geom_line(aes(y=AHDD, col = "AHDD")) +
  scale_x_date(labels=date_format("%m/%Y"), breaks = date_breaks("3 months"))  

# 3. Add a legend and rotate the text of the x-axis. Assign it to w2.
w2 = w1 + scale_colour_manual(name="",values=c("ACDD" = "blue","AHDD" = "red")) +
  theme(axis.text.x=element_text(angle = 40, vjust = 0.7))

# 4. Call the variable to plot it.
w2
#>
```

In general the plot shows the variability of the weather. When you take a closer look at the graph you can see three prevailing differences between both lines of the weather variables. First of all ACDD is much higher than ACDD. This fact we have already observed in previous tasks where we calculated the mean value and the maximum of both weather variables. Second, ACDD are much more frequent than AHDD. This indicates that heating is less important in Florida. The third difference concerns the seasons of the year. ACDD peaks in the summer months and during the winter it is really close to zero. AHDD behaves exactly the opposite. Which you might have already expected. 
In conclusion, we see a high variability of the weather variables and these changes in the weather are the main reasons of fluctuations in the residential energy consumption. Keep this relationship in mind as we will come back to the weather variables and their influence on the energy consumption in exercise number 4.

*Exercise 1.3 refers to the pages 9, 10 and 16 of the paper and to table 1.*


## Exercise 2 -- Building code change in more detail

In this exercise we will more precisely analyse the differences of residences built before and built after the code change. We will take a look at the energy consumption and the housing characteristics of the residences. 

But first of all you get some further information about the building code and about the changes of it.  
<br> Most U.S. states have state-wide building energy codes which regulate the energy efficiency of newly built residences. The aim of the building codes is a reduction of the energy consumption of the buildings and consequently also a decreasing carbon dioxide emission in recognition of the climate change.
<br> Now we will focus on the building code which has been established in Florida. It states a minimal standard for the overall energy efficiency of a newly built residence compared to a baseline home. That means, certain components of the new building are allowed to use more energy than the baseline home but in total the new house has to be more efficient. The building code determines the characteristics of the baseline home. Hence, a tightening of the building code leads to a more energy efficient baseline home.
<br> We will closely look at the established code change in March 2002 and at its effects. Florida Building Commission implemented three major and several smaller changes to the building code. The most important change was the reduction of the Solar Heat Gain Coefficient from 0.61 to 0.4. This coefficient is "the amount of solar heat that actually enters the window compared to the amount that strikes it on the outside" (according to: http://www.energygauge.com/FlaRes/new_code.htm). 

*Further information about the building code change you can get directly from the paper. You may take a look at page 1 and page 5 f. Additional you can find more details on the following page: http://www.energygauge.com/FlaRes/new_code.htm.*

Now we know a lot more about the building code in Florida. In the following exercises we will analyse if the building code change leads to a reduced actual energy consumption. And we additionally look at the effects of the code change on the housing characteristics.   
<br> In more detail, in exercise 2.1 we calculate the mean values of the energy consumption and the housing attributes separately for the residences built before and built after the code change in order to compare them. We also plot these values to get a first idea of the effects of the code change.  
<br> In exercise 2.2 we will perform a two sample t-test to support our findings of exercise 2.1.

*This exercise refers to page 10 and table 2 of the paper.*

## Exercise 2.1 -- Analysis of the mean values 

<!-- only mean values, standard deviation not (one calculation, but no interpretation)-->

In this exercise we get a first idea of the effects of the code change. Therefore, we calculate the mean values of the energy consumption and the housing attributes separately for the residences built before and built after the code change. But before we are able to calculate these values we have to several steps to prepare the data. One important step is to separate the data frame into two parts, one containing the data of all pre-code residences and the other containing the data of all residences built after the code change. 
<br> Then we calculate the mean values and compare them. We will also draw some plots to better visualise the differences.

First of all we have to load the data `codechange.dta`. Remember the last exercise how to do this. If you need help you can press on the `hint` button.
*Because you have started a new exercise you have to click on the `edit` button to be able to enter your code in the code box.*

**Task:** Load the package `foreign` and read in the data `codechange.dta` and assign it to `dat2`. Print the first lines of the data frame by using the command `head()`. 
```{r "2_1"}
#< task
# load the package 
# enter your code here..  
#>
library(foreign)

#< hint
display("Use the command library() to load the package. The name of the package is foreign.") 
#>

#< task
# read the data frame codechange.dta into R 
# enter your code here..  
#>
dat2 = read.dta("codechange.dta")

#< hint
display("To read in the data your command should have the following form: 
        dat2 = read.dta(\"???\"). Replace the ??? by the name of the data set you want to load.")
#>

#< task
# show a part of the data frame dat2 
# enter your code here..  
#>
head(dat2)

#< hint
display("Enter head(???). Replace the ??? with the name of the data frame.") 
#>
```

<br>Now we can work with the data frame `dat2`. This data frame has $12$ columns containing variables describing the energy consumption and some housing characteristics. Additionally two columns characterize the code regime, i.e. if the residence was built under the rules of the old building code or under the regulations of the new and more tightened one.      

We want to divide this data frame into two subsets, one covering the pre-code buildings and the other the post-code buildings. After that we will calculate several values and then compare both groups.

Before we divide the data frame we have to do several steps to prepare the data.
<br>First, the authors Jacobsen and Kotchen only used the billing data of the year $2006$ (and not for the years from $2004$ to $2006$) to determine the difference of the pre-code and post-code residences. Hence, we will also just consider the data of the year $2006$ to receive the same results as in the paper. To perform this step in the R code we use the function `filter()` out of the `dplyr` package and choose the condition `year == 2006`.  
<br> Second, the data frame should contain only one observation per house. This means the number of rows of this data frame should be $2239$ because we observe $2239$ different residences. Every household is characterized by an identification number. This variable is called `home_id`. 
<br> Last, to generate one observation per house we need a function that pools the monthly values together to only one value. In our case the pooling function should be the `mean` function. In an previous task you used the function `summarise()` for this purpose. But in this task we need a different one because we have a large number of columns and it would be really effortful to write each column name separately. This function we will use is called `summarise_each()`. Inside the brackets you have to define the functions you want to apply on the columns. In our case we want to apply the function `mean` so we write `funs(mean)` inside the brackets.
<br> If you like to read more about this function you can take a look at https://cran.r-project.org/web/packages/dplyr/dplyr.pdf. 

**Task:** Prepare the data frame `dat2` like described above and assign it to `dat_house`. You have to replace the ??? by the correct commands. Remove the comments to check your solution.   
```{r "2_1__2"}
#< task
# load the package dplyr
library(dplyr)
# Replace the ??? with the correct commands and then uncomment the code.

# dat_house = ??? %>% 
#   ???(year == 2006) %>%
#   group_by(home_id) %>%
#   summarise_each(funs(mean))
#>

dat_house = dat2 %>% 
  filter(year == 2006) %>%
  group_by(home_id)  %>%
  summarise_each(funs(mean))

```

*This task can be solved optionally.*

**Task:** You may like to take a look at the prepared data frame. If you like you can use the command `head()` to show the first lines of `dat_house`.  
```{r "2_1__3",optional=TRUE}
#< task
# enter your code here...
#>
head(dat_house)
```

<br>Because we are interested in the differences in the energy consumption and the housing characteristics of the pre-code and post-code buildings we have to divide the prepared data frame `dat_house` into two groups. 
<br> To separate the data frame we use the variable `code_change`. This variable is connected to the variable `EYB` and represents the building code change. It is an indicator variable, i.e. it takes only the values $0$ and $1$. In more detail:
  - `code_change == 0` is equivalent to residences with an `EYB` of 2001 or earlier and therefore associated with the residences built before the code change and
  - `code_change == 1` is related to buildings with an `EYB` of 2003 or later and therefore associated with the post-code residences.
  
Now we do the calculation and separate the data frame into two groups by using the variable `code_change`. You already performed similar computations in exercise 1.1 so you now only have to press `check`. 

*If you didn't solve the last task you first have to click on the `edit` button to be able to enter your code.*
```{r "2_1__4"}
#< task
# form a data frame which contains all residences built before the code change
pre_code  = filter(dat_house, code_change == 0)

# form a data frame which contains all houses built after the code change
post_code  = filter(dat_house, code_change == 1)
#>
```

We have to do another step of preparation the data frame before we can start with calculating the values. We are not interested in all variables of the two data frames `pre_code` and `post_code` so we have to remove some columns. More exactly, we only want to consider the following $8$ variables: `elec`, `gas`, `sqfeet`,`baths`, `beds`, `stories`, `centralair` and `shingled`. Hence, we have to remove $4$ columns of our data frames. One way to do this is to use the `select()` function out of the `dplyr` package with a minus in front of the variable name. For example, the command `select(dat, -year)` creates a new data frame which is a subset of the data frame `dat` without the column `year`. You get more information about this function on https://cran.r-project.org/web/packages/dplyr/dplyr.pdf.

**Task:** Remove the columns `home_id`, `year`, `code_change` and `EYB` from the data frames `pre_code` and `post_code`. Use the command `select()` and assign the new data frames to `pre_code2` and `post_code2` respectively.  
```{r "2_1__5"}
#< task
# replace the ??? by the correct commands and remove the comment
# pre_code2 = ???
#>
pre_code2 = select(pre_code,-home_id, -year, -code_change, -EYB)

#< hint
display("Your command should look like:
pre_code2 = select(dat_name,-col_name1, -col_name2, -col_name3, -col_name4). Insert the correct names of the data frame and columns you want to remove.")
#>

#< task
# replace the ??? by the correct commands and remove the comment
# post_code2 = ???
#>
post_code2 = select(post_code,-home_id, -year, -code_change, -EYB)

#< hint
display("The command is similar to the one above. You only have to change the name of the data frame.")
#>
```

#< award "dplyr user level 3"
Great! You are more and more familiar with the dplyr package. Now you know a lot of this package and you hopefully see how important this package is for preparing data frames.
#>

All preparations are done so far. Now the data frames cover the necessary variables representing the energy consumption and some housing characteristics. Hence, in order to compare the residences built before the code change and built after the code change we adduce the mean value and we additionally calculate the standard deviation of all variables. 

So let us compute these values in the following task. Again we use the function `summarise_each()` to perform the calculations for each column separately. The function `sd` calculates the standard deviation. We save the resulting vectors as `beforeCC` (if we used the data frame `pre_code2`) and `afterCC` (otherwise). Press `check` to perform the calculations.  
```{r "2_1__6"}
#< task
# calculate the mean values and the standard deviations of all 8 variables of the 
# prepared pre-code data frame
beforeCC = summarise_each(pre_code2, funs(mean, sd))

# calculate the mean values and the standard deviations of all 8 variables of the 
# prepared post-code data frame
afterCC = summarise_each(post_code2, funs(mean, sd))
#>
```

Both vectors contain $16$ different values. The first $8$ values are the different mean values and the last $8$ are the standard deviations of the variables.
<br>We round all numbers to three digits after the decimal point to make them easier to read. Just press `check`.   
```{r "2_1__7"}
#< task
# rounded values of the pre-code data frame 
BCC = round(beforeCC, digits=3)
# show the vector BCC (short for: before code change)
BCC 

# rounded values of the post-code data frame
ACC = round(afterCC, digits=3)
# show the vector ACC (short for: after code change)
ACC
#>
```

<br>Now let us take a look at the resulting values. It is really helpful to visualise these values in order to determine the differences. Hence, we draw some plots.
 
We only want to plot the mean values and we will generate a graphic with smaller plots for each variables so we have to prepare the data frames. First we select the corresponding columns and second we will transform the data into a clear form. Each row should represent one observation and each columns should contain the values of one variable. The function `gather()` out of the package `tidyr` transforms the data from a "wide" format to a "long" format. Take a look at the **info box** below the code chunk if you need further information about this function and if you like to see the resulting data frames of the following code chunk.

Click on the button `check` to prepare the data frames and perform the changes. 
```{r "2_1__8"}
#< task
# prepare the data frames: only consider mean values
BCC_mean = select(BCC, ends_with("mean"))
ACC_mean = select(ACC, ends_with("mean"))


# prepare the data frames: change long format into wide format
# load the package
library(tidyr)

# transfrom the data
BCC_plot = gather(BCC_mean)
ACC_plot = gather(ACC_mean)
#>
```

The **info box** below contains further information about the function `gather()` and it also shows a transformed data frame. 

#! start_note "package tidyr and function gather()"
The package `tidyr` contains different functions which transform the data into a clear and simple format. 
<br> One important function out of this package is the function `gather()`. It collapses several columns into key-value pairs such that each row represents one observation and each column contain the values of a variable. Hence, this function transforms the data from a "wide" format into a "long" format. This means, the resulting data frame only has fewer columns but a lot more rows than the original data frame.  
<br> You can receive more information about this package and its function on https://cran.r-project.org/web/packages/tidyr/tidyr.pdf.

To illustrate these explanations we take a look at the transformation of our data frame we performed in the previous code chunk. We choose the data frame `ACC_mean` of our post-code residences. This data frame is saved in wide format with 8 different columns. Press `check` to show the data frame. 
```{r "2_1__9",optional=TRUE}
#< task
ACC_mean
#>
```

<br>Then we used the function `gather()` to transform the data frame into long format. The resulting data frame is called `ACC_plot`. Click on the `check` button to show the data frame. 
```{r "2_1__10",optional=TRUE}
#< task
ACC_plot
#>
```
<br>Now you can clearly see the transformation. The resulting data frame `ACC_plot` has 2 columns and 8 rows. The column `key` contains the different variable names and the column `value` contains the different mean values.
#! end_note

All preparations of the data frames are done. So we can now plot the data. We want to plot the mean values of the energy consumption and the housing characteristics of the pre-code and of the post-code residences in order to compare them. Each of the $8$ different variables should belong to a separate plot and the corresponding mean values should be drawn inside the smaller plots. All $8$ smaller plots together should form the whole graphic.       
Again we use the functions of the package `ggplot2`. This time we need several functions to specify the features of the plot so the code is more complex. There are some comments written in the code box. You can find a more exact description of the code lines in the **info box** below. 

#< info "Description of code lines for plotting the mean values"
This **info box** gives detailed information about the code which creates the graphic for the mean values of the different variables and the pre- and post-code residences.
<br> Again we use the functions of the package `ggplot2`. In this case we only want to plot a few discrete values so we need the function `geom_point()`.
<br> We want to plot the mean values on the y-axis and the different variables on the x-axis, therefore we write `aes(x=key, y=value)`. We can't specify the data frame in the command `ggplot()` because we have two different data frames. So we define `data=dat_name` in the command `geom_point()`. In the first step we already add the titles of the x-axis and the y-axis. 
<br> To differ between the pre-code and post-code values we use different colours `colour = ??? ` and shapes `shape = ??? `. We have to specify both arguments manually with the functions `scale_colour_manual()` and `scale_shape_manual()` to be able to add a legend. The different shapes are represented by numbers from 1 to 25. A table of the corresponding point shapes and numbers can be found here: http://www.cookbook-r.com/Graphs/Shapes_and_line_types/. The argument `size=2` enlarges the points. 
<br> In addition we don't want one great graphic but several smaller plots next to each other. Every variable (saved in the column `key` of the data frames) should be contained in one smaller plot. To get such a graphic we use the function `facet_grid(. ~ key)`. The arguments `scales = "free" ` and `space = "free" ` allow varying scales and panel sizes. 
<br> Last but not least we want to add a legend. We use the two functions `scale_colour_manual()` and `scale_shape_manual()` which I have already described above. We additionally use the command `theme(legend.position = "bottom")` to change the position of the legend from the left side to the bottom. The arguments `axis.text.x = element_blank()` and `axis.ticks.x = element_blank()` are used to get rid of the axis ticks and the text of the x-axis. In this graphic they are not necessary because every subplot only contains one variable and already has a title. 

*references: https://www.rstudio.com/wp-content/uploads/2015/06/ggplot2-german.pdf, http://docs.ggplot2.org/0.9.3.1/facet_grid.html and http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/*
#>

Just press `check` to create the plot.    
```{r "2_1__11",fig.height=5, fig.width=9}
#< task
# load the package
library(ggplot2)

# create the basic level with titles for the x-axis and y-axis
p = ggplot(NULL, aes(x=key, y=value)) + ylab("mean values") + xlab("variables")

p1 = p + 
  # add the mean values of the pre-code data frame 
  geom_point(data=BCC_plot, aes(colour="before code change", shape="before code change"),
             size=2) + 
  
  # add the mean values of the post-code data frame 
  geom_point(data=ACC_plot, aes(colour="after code change", shape="after code change"), 
             size=2) +   
  
  # add panels, i.e. split the plot into several parallel subplots  
  facet_grid(. ~ key, scales="free", space="free") 

# add a legend with manually declared points (colour and shape) and specify its position
p2 = p1 + 
  scale_colour_manual(name="", values=c("before code change"="blue",
                                        "after code change"="red")) +
  
  scale_shape_manual(name="", values=c("before code change"=4,"after code change"=3)) +
  
  theme(legend.position="bottom", axis.text.x=element_blank(), 
        axis.ticks.x=element_blank()) 

# assign the variable to plot it
p2
#> 
```

The graphic shows the mean values of the pre-code buildings (coloured in **blue**) in comparison to the post-code residences (coloured in **red**) separately for $8$ different variables. It is difficult to detect the differences of the particular variables because most of the points are lying really close to each other. Only the mean values of the electricity consumption and of the square footage differ visibly and we can see that the mean values of the pre-code residences are higher than the ones of the post-code buildings. But for the remaining six variables we can not make a point. Therefore, we want to use another format with different scales on the y-axes. 
<br> We get this result by using the function `facet_wrap(~ key)` instead of `facet_grid(. ~ key)`. We specify `ncol = 4` to receive a graphic with $2$ rows of respectively $4$ smaller plots next to each other. Further information about the function `facet_wrap()` you can read here: http://docs.ggplot2.org/0.9.3.1/facet_wrap.html.

All other commands left unchanged. Just press `check`. 
```{r "2_1__12",fig.height=8, fig.width=9}
#< task
q = ggplot(NULL, aes(x=key, y=value)) + ylab("mean values") + xlab("variables")

q1 = q + 
  geom_point(data=BCC_plot,aes(colour="before code change", shape="before code change"), 
             size=2) +
  
  geom_point(data=ACC_plot, aes(colour="after code change", shape="after code change"), 
             size=2) +
  
  # use this function to get a different format of the graphic
  facet_wrap(~ key, ncol=4, scales="free")

q2 = q1 + 
  scale_colour_manual(name="",values=c("before code change"="blue", 
                                       "after code change"="red")) +
  
  scale_shape_manual(name="", values=c("before code change"=4,"after code change"=3)) +
  
  theme(legend.position="bottom", axis.text.x=element_blank(), 
        axis.ticks.x=element_blank()) 

q2
#>
```

Now you clearly see the differences of the mean values. But be careful when looking at the graphic because every smaller plot now has a different scale on the y-axis. Even if the points seem to be far away the difference in absolute values can be really small.

Now you should be able to answer the questions of the following two quizzes.

#< quiz "Comparison of pre-code and post-code residences No. 1"
question: Which statement about the energy consumption of the residences is correct? 
sc: 
    - Post-code residences consume more energy than pre-code residences. 
    - The natural gas consumption of post-code buildings is smaller but the energy consumption is higher in comparison to residences built before the code change.
    - Houses built after the building code change need less energy than houses constructed before the code change.*
    
success: Great, your answer is correct!
failure: Not correct answered. Try again.
#>

In general, our data indicates that post-code residences consume less energy than pre-code buildings. Now we will look at the difference of the energy consumption in more detail.

You can enter some code in the box below to answer the questions or you can do mental arithmetic.
```{r "2_1__13",optional=TRUE}
#< task
# enter your code here...
#>

#< hint
display("Performing following calculations will help you to solve the questions: 
        for the 1. BCC$elec_mean - ACC$elec_mean, 
        for the 2. BCC$gas_mean - ACC$gas_mean.")
#>
```

#< quiz "Comparison of pre-code and post-code residences No. 2"
parts:
  - question: 1. How much kWh of electricity do post-code residences use less on average than houses built before the code change? Round the number to two digits after the decimal point.  
    answer: 44.85
    roundto: 0.01

  - question: 2. How big is the absolute difference of natural gas consumption of pre- and post-code houses, measured in therms? Round the number to one digit after the decimal point. 
    answer: 5.8
    roundto: 0.1

success: Great, all questions correctly solved!
failure: Not all questions solved correctly. Try again.
#>

We observed the differences of the energy consumption. Now we want to look at the housing characteristics. Theses variables do not really differ between the two groups as we already saw in the first graphic, with one exception: residences built after the code change are a lot smaller than houses built before the code change (measured in the variable `sqfeet`). In numbers, the difference is about $4.5 \%$. 

*Exercise 2.1. refers to page 10 and table 2.*

## Exercise 2.2 -- Statistical analysis by using a two sample t-test

In the previous tasks we looked at the mean values of the energy consumption and several characteristics of the pre-code and post-code residences. Our results were the following: the residences built after the code change use less energy (i.e. electricity and natural gas) than the pre-code buildings and additionally post-code residences are smaller than pre-code buildings. In this exercise we want to verify these results based on a statistical test. We use the two sample t-test. 
<br> First of all you get some theoretical background information about the t-test. Then you will see how it works to implement this test with R. Last but not least you can solve some tasks and apply your knowledge to our data. 

We will use the **two sample t-test**. This test checks wether the mean values of two groups or samples are equal or different to each other. 
<br> In our case we will test if the characteristics of the pre-code residences are smaller or equal than the characteristics of the residences built after the code change. The two groups are independent. Let us assign the pre-code residences to **group 0** (with mean value $\mu_0$) and the post-code residences to **group 1** (with mean value $\mu_1$). 
<br> We additionally define the difference $diff$ of the mean values as: $diff = \mu_0 - \mu_1.$  
Hence, we test whether the null hypothesis $$H0: diff \le 0$$ holds true. Against the alternative hypothesis $$H1: diff > 0.$$ 
<br>To be more precise, we test a one-sided (in our case left-sided) null hypothesis (according to: Weerahandi (1995), p. 29).

The test statistic **TS** of a two sample t-test is $$TS = \frac{(\overline{\mu_0}-\overline{\mu_1}) - 0}{SE_{pooled}(\overline{\mu_0}-\overline{\mu_1})}\ \sim t_{m+n-2} ~~~(if~ H0~ holds~ true)$$ if both samples have the same variance. $\overline{\mu_0}$ and $\overline{\mu_1}$ are estimators of $\mu_0$ and $\mu_1$, respectively and $SE_{pooled}(X)$ denotes the pooled standard error of X (according to: Stock and Watson (2007), p. 83f., 88 - 92). We will show the equality of variance of both samples in the **info box** below.
<br>If H0 holds true, **TS** is Student-t-distributed with $m+n-2$ **degrees of freedom (df)** (reference: Stock and Watson (2007), p. 89, 91), where $m$ is the sample size of group1 and $n$ the sample size of group2, under the assumption that H0 holds true. In our case it holds $m = 1293$ and $n = 946$, hence $df = 2237$. This means, we have $2237$ linearly independent sample observations which we use for the calculation of the test statistic (according to Kennedy (2008), p. 505). 
<br>Because the sample sizes of both groups are large we can conclude with the central limit theorem (CLT) that both samples are approximately standard normal distributed (according to Stock and Watson (2007), p. 77, 83f., 89 and 91f.). More information about the CLT you can find in Stock and Watson (2007), p. 52-55.
<br>Furthermore, we denote a realisation of the test statistic TS by **ts**. 

To test if the hypothesis is true we need two additional values: first, the **p-value** and second, the **significance niveau** $\alpha$. The **p-value** is the probability to obtain a realisation of the test statistic which is such or more extreme than its observed value, under the assumption that H0 holds true ( Kennedy (2008), p. 508 and Weerahandi (1995), p. 29). You can compute the p-value in this case by using the following formula: $$p = Pr(|TS| \ge ts ~|~ H0 ~holds ~true)$$ (according to Weerahandi (1995), p. 30f.).
<br>We assume the (typical) confidence level of $0.95$. This means, the **significance level** $\alpha$ is equal to $0.05$ ($\alpha = 0.05$) (according to Stock and Watson (2007), p. 79).

We can decide if the null hypothesis holds true or not by comparing the significance niveau with the p-value. A small p-value indicates that the data doesn't support the null hypothesis well but favors the alternative hypothesis. Hence, if the $p$-$value$ is smaller than $\alpha = 0.05$ we can reject the null hypothesis $H0: diff <= 0$ at a significance niveau of $5 \%$ (according to Weerahandi (1995), p. 30f. and 110ff.). 

Now you are familiar with the background of/theory behind the t-test and so we will now perform this test with our data. But before we are able to do this we have to load the data because we have started a new exercise. We have already prepared the data frame in the previous exercise so we just import the two data frames which were called `pre_code2` and `post_code2`. We now assign the data frames to `pre_code` and `post_code` respectively.  
<br> First press `edit` and then `check` to load the data.
```{r "2_2"}
#< task
# load the package foreign
library(foreign)

# load the two data frames
pre_code = read.dta("pre_code2.dta")
post_code = read.dta("post_code2.dta")
#>
```

You can take a look at the **info box** below if you want to know if our samples have the same variance.

#! start_note "check equality of variances"
We assumed that the variances of our two samples are equal. Now we want to verify our assumption and check the equality or homogeneity of variances of our two groups.  
<br> There are several tests possible but we will use a visual verification by drawing two boxplots. You can read more details about these different tests on http://www.cookbook-r.com/Statistical_analysis/Homogeneity_of_variance/.

We exemplarily test the equality of variances for the electricity consumption and thus we use the variable `elec`. We additionally have to specify the data frames, which are called `post_code` and `pre_code`. 
<br> First, we use the function `par()` to set the graphical parameters. For the purpose of a better comparison we arrange the two boxplots next to each other and therefor we use the argument `mfrow=c(1,2)`. Then we draw the boxplots. We specify the data frames, the titles and the values of the y-axes. Press `check` to see the solution.    
<br> If you like to read more about the different R functions you can take a look at http://www.statmethods.net/advgraphs/layout.html, http://stat.ethz.ch/R-manual/R-devel/library/graphics/html/boxplot.html and http://docs.ggplot2.org/0.9.3/xylim.html.

```{r "2_2__2",optional=TRUE}
#< task
# arrange boxplots next to each other 
par(mfrow=c(1,2))

# draw the boxplots
boxplot(pre_code$elec, main ="pre-code residences", ylim = c(0,6000))
boxplot(post_code$elec, main ="post-code residences", ylim = c(0,6000))
#>
```
<br>Both boxplots appear to be equal. Hence, we can conclude that the variances of both samples are equal. <br>In this problem set the visual verification suffices but if you want to get a really reliable result you should perform additional tests. 
#! end_note

Now we want to perform the two sample t-test with our data. The corresponding R function is called `t.test()`. You may take a look at the **info box** below to get further information about this function and its arguments. If you are still familiar with this function you can directly solve the following tasks.

#< info "t.test()"
We want to perform a t-test. The corresponding R function is called `t.test()` with several arguments. Just take a look at the following code box. 
```{r "2_2__3",optional=TRUE, eval=FALSE}
# t test with default options
t.test(x, y = NULL, alternative = c("two.sided", "less", "greater"), mu = 0, 
       paired = FALSE, var.equal = FALSE, conf.level = 0.95)
```

We have to specify some arguments to perform an appropriate test for our data. First of all, we have two samples `pre_code` and `post_code` and not only one. So we have to assign x and y to the corresponding names. Because we want to test several variables of the two groups and not only the samples itself we have to define the sample and the variable name. For example `post_code$elec`. <br>Second, we have to use the alternative $greater$ because we defined our alternative hypothesis H1 as $H1: diff > 0$. `diff` is conform to the argument `mu`. 
<br>Last, we have equal variances so we set `var.equal = TRUE`. If the variances of both samples are not equal the Welch approximation/test will be used.  

All other arguments can be left unchanged. You see the changes in the below code box. We additionally can leave out all default options to get a function call which is more shortly and therefore also more clearly. 
```{r "2_2__4",optional=TRUE, eval=FALSE}
# t test with specified options
t.test(x = pre_code$var_name, y = post_code$var_name, alternative = "greater", mu = 0, 
       paired = FALSE, var.equal = TRUE, conf.level = 0.95)

# t test with specified options and default options left out
t.test(x = pre_code$var_name, y = post_code$var_name, alternative = "greater", 
       var.equal = TRUE)
```

*references: https://stat.ethz.ch/R-manual/R-devel/library/stats/html/t.test.html* 
#>

**Task:** Test if the average electricity consumption of pre-code and post-code buildings is equal to each other. Use a two sample t-test. The code is already given. Just press `check`.
```{r "2_2__5"}
#< task
test1 = t.test(pre_code$elec, post_code$elec, alternative = "greater", var.equal = T)
# show the output
test1
#>
```

You can see the test results in the code box above. The next step is to interpret these results. Important for us are the following three values: the **t-statistic** ($t = 1.7934$), the **p-value** ($p$-$value = 0.03652$) and the **confidence level** ($1-\alpha = 0.95$).
<br> We compare the significance niveau with the $p$-$value$ to make a statement if we reject the null hypothesis or not. The $p$-$value$ of our test is equal to $0.03652$. Hence, our $p$-$value$ is smaller than $\alpha = 0.05$. So we can reject the null hypothesis $H0: diff <= 0$ at a significance niveau of $5 \%$. Therefore, our assumption that pre- and post-code residences have the same electricity consumption on average was false. We can accept the alternative hypothesis and say that pre-code residences consume more electricity than post-code residences.

**Task:** Perform another two sample t-test. Check whether the null hypothesis "the average natural gas consumption of residences built before and after the code change is equal to each other" can be rejected or not. Save the test results in the variable `test2`. Replace the ??? by the correct code and then uncomment this line. Afterwards press `check`. If you need some help press `hint`. 
```{r "2_2__6"}
#< task
# perform the t-test
# test2 = ???
#>
test2 = t.test(pre_code$gas, post_code$gas, alternative = "greater", var.equal = T)

#< hint
display("Your command should look like: 
        test2 = t.test(pre_code$gas, post_code$gas, alternative = \"???\", var.equal = ???). Replace the ??? by the correct arguments. You can find futher information in the info box.")
#>

# show the results
test2
```

#< award "hypothesis tester"
Great! You performed a two sample t-test on your own! 
You are able to use a statistical test to check whether your hypothesis was right or wrong.
#>

While looking at the test results we recognize that the p-value is $1.182e$-$15$ and approximately $0$. So it is below the significance level $\alpha = 0.05$. Hence, we can reject the null hypothesis and conclude that after-code-change buildings use less gas for heating than houses built before the code change.

Summarising the results of both tests we have strong evidence for the statement that post-code residences consume less energy. Because post-code buildings need on the one hand less electricity for cooling and on the other hand less natural gas for heating. So we could support the results of exercise 2.1.

Now we consider the housing characteristics. We want to know whether the energy code change also lead to statistically significant changes in the mean value of these variables. We do this analysis exemplarily on the basis of two attributes, namely the square footage (`sqfeet`) and the number of bedrooms (`beds`).
We only expect significant changes in the variable `sqfeet` because of the results of the previous exercise. In the previous exercise we only saw  visibly differences in the mean values of the variable `sqfeet`. The mean values of all other housing characteristics were nearly the same. 

**Task:** Run two sample t-test for the variables `sqfeet` and `beds` of the samples `pre_code` and `post_code`. Replace the ??? in the code with the correct commands and remove the comments. Then press `check`.
```{r "2_2__7"}
#< task
# two sample t-test for the variable sqfeet
# test3 = ???
# test3
#>
test3 = t.test(pre_code$sqfeet, post_code$sqfeet, alternative = "greater", var.equal = T)
test3

#< task
# two sample t-test for the variable beds
# test4 = ???
# test4
#>
test4 = t.test(pre_code$beds, post_code$beds, alternative = "greater", var.equal = T)
test4
```
<br>
#< quiz "interpretation of test results"
question: Consider the results of test3. Denote the correct statement. 
sc: 
    - In this case the null hypothesis can not be rejected at a significance level of 5 %. 
    - Post-code buildings are smaller than pre-code buildings significant at the 5 % niveau.*
    
success: Great, your answer is correct!
failure: Not correct answered. Try again.
#>

<br>
#< quiz "further interpretation of test results"
question: Look at test4 and the results. Denote the correct statement.
sc:
   - The p-value is smaller than alpha.
   - The null hypothesis can not be rejected at the 5 % niveau.*
   - The null hypothesis "the average number of bedrooms of pre- code houses is greater or equal than the number of post-code residences" is true.
success: Great, your answer is correct!
failure: Not correct answered. Try again.
#>

#< award "interpreter of test results"
Great! You are able to interpret the test results of a t-test correctly and you can decide if the null hypothesis can be rejected or not.
#>
<br>We don't perform t-tests for the other housing characteristics because the results are either not statistically significant or the differences are really small. As we already saw in the plots of exercise 2.1.
<br>Summarising, in this exercise we performed four different t-tests. Upon the results of $test1$ to $test3$ we could reject the null hypothesis of each test which supported our expectations (of exercise 2.1). So we can conclude that post-code residences use less electricity and natural gas and that they are smaller than residences built before the code change. Hence, building codes lead to a reduced energy consumption. 
<br> We will support this result by further statistical analyses in the following exercises 3 and 4.

*This exercise refers to the page 10 and table 2 of the paper.*

## Exercise 3 -- Empirical analysis: pre- and post-code change comparisons

In this exercise we will start with the empirical analysis to determine the effects of building codes on the actual energy consumption of Gainesville residences. 
<br> The authors of the paper used two different empirical strategies: firstly, a pre- and post-code change comparison and secondly a difference-in-differences analysis. This exercise deals with the first strategy and exercise 4 will consider the second strategy. 

Our aim is to calculate estimates for the differences in electricity and natural gas consumption between the residences built before and after the code change. First we have to set up a **linear regression** model and then we will estimate the differences using **ordinary least squares (OLS) estimation**. We will consider two specifications of this regression model. In exercise 3.1 we will calculate estimates for the annual differences in the energy consumption and then in exercise 3.2 we will expand the model to get estimates for the monthly differences. In exercise 3.3 we discuss possible limitations of our chosen regression model. 

But before we go on to the next exercise you will get some information about (linear) regression theory. This basic knowledge is important for understanding the following exercises and also to be able to perform the calculations with our data in R.

(transition)

If the regression model fulfills the following **five assumptions** we consider the OLS estimator to be the "optimal" estimator, this is a consistent, unbiased and efficient estimator (for more details see: Kennedy (2008), p.12-18 and p.40-44).

#< info "OLS estimator"
The ordinary least squares estimator, or short **OLS estimator** is a possible estimator for estimating the unknown coefficients of/in linear regression models. It generates estimates by **minimising the sum of squared residuals**.
<br> To understand the meaning of this we first set up a linear regression model: $$y = X \cdot \beta + \epsilon.$$ $Y$ is the dependent or response variable and $X$ a matrix containing the explanatory variables. The last parameter of this equation is the disturbance $\epsilon$.
<br> Second, we give a definition for the residuals which are the estimated values of the disturbance $\epsilon$. As formula $\hat\epsilon = y - \hat{y} = y - X \cdot \hat\beta$ with $\hat\beta$ being the estimate of the true coefficient vector $\beta$ and $\hat y$ being the predicted or fitted values of y.
<br> Last, the formula for the OLS estimate is $$\hat\beta = argmin \sum^T_{t=1} \hat\epsilon^2$$ with $t=1,...,T$ being the number of observations.

*reference: Kennedy (2008), p.12f., Weerahandi (1995), p. 14 - 17*
#>
  
Let $t=1,...,T$ be the number of observations.

**Assumption 1:** The dependent variable $Y$ is a linear function of a set of $K$ independent variables $X$ and of a disturbance term $\epsilon$. Let's call the unknown coefficients $\beta_i$ ($i = 0,...,K$) which form the vector $\beta$. So the mathematical formula of this multivariate regression model is: $$Y = X \cdot \beta + \epsilon$$ with $Y = (y_1,...y_T)$, $\epsilon = (\epsilon_1,...,\epsilon_T)$ and $X$ being a $\textrm{K+1} \times \textrm{T-matrix}$.
<br>**Assumption 2:** The expected value of the disturbance $\epsilon$ is $0$: $E[\epsilon] = 0$.
<br>**Assumption 3:** The disturbance terms have same variance and are uncorrelated.
<br>**Assumption 4:** The observations on the explanatory variables can be "considered fixed in repeated samples".
<br>**Assumption 5:** The number of observations is greater than the number of explanatory variables ($T>K$) and there is no "exact linear relationship between the independent variables."
<br>*reference: Kennedy (2008), p. 40 - 44*

Violations of one of the assumptions lead to different problems and we may get an inconsistent or unbiased estimator. If you like detailed information you can look at Kennedy (2008), p. 41ff. and (chapter 6 to 12; p. 93 - 202) . In this problem set we only take a closer look at **assumption 3**. A typical violation of this assumption is if we have autocorrelated error terms. Autocorrelation means "the disturbance term for one observation is correlated with the disturbance term for another observation" (Kennedy (2008), p. 113). If we have autocorrelated error terms it follows that our OLS estimator is no longer efficient. 
<br> In our case the error terms are correlated because of clustering. Each residence belongs to a different cluster. Hence, all observations of one residence form a cluster. To be more precise, the energy consumption of households is influenced by the housing characteristics of their residence and therefore the errors of one residence are correlated with each other (according to: Kennedy (2008), p. 112f. and 118ff. and 128; paper, p. 11f.). 
<br> We handle this problem of autocorrelated error terms by using clustered standard errors (refer to paper, p. 11f.). You will get more details in the following exercise.

#< info "autocorrelated standard errors"
We already mentioned that we have correlated error terms. Now we proof the autocorrelation by using the Durbin-Watson test. More details you can find in Kennedy (2008), p. 118ff.
We need the function `dwt()` out of the package `car` to perform this test with R. You may take a look at https://cran.r-project.org/web/packages/car/car.pdf for more information. This time we specify the linear model with the function `lm()`. The regression formula will be explained in the following exercise. Now we are only interested in the results of the Durbin-Watson test.

```{r "3",optional=TRUE, eval=FALSE}
# load data
library(foreign)
dat=read.dta("CCC_regression1.dta")

# load package
library(car)
# perform Durbin-Watson test
dwt(lm(elec ~ code_change + logfeet + centralair + shingled + factor(baths) + factor(beds)
       + factor(stories) + factor(zip) + factor(monthyear), data=dat))
```

The following code box shows the results.
```{r "3__2",optional=TRUE, eval=FALSE}
# result:
lag Autocorrelation D-W Statistic p-value
 1       0.7837161      0.432541       0
Alternative hypothesis: rho != 0
```

The null hypothesis is reflecting no autocorrelation with a **D-W Statisitic** of approximately $2$. In our case this statistic is really low and the p-value is $0$ so we can reject the null hypothesis. Hence, we conclude that our error terms are correlated and so our assumption of autocorrelation was correct. 

*reference: Kennedy (2008), p. 118ff.*
#>

*This exercise refers to the pages 10 to 16 and table 3 and figure 1 to 4 of the paper.*

## Exercise 3.1 -- Annual differences

In this exercise we will calculate the estimates for the annual differences in electricity (section 1, part a) and natural gas (section 1, part b) consumption between the pre- and post-code buildings. In section 2. we use an alternative specification of the regression model to test for robustness of the results.

First let us state our regression model. We call it **model 1**: $$Y_{it} = \delta \cdot code\_change_i + \beta \cdot X_i + v_t + \epsilon_{it}.$$ The equation contains the following parameters: $Y_{it}$ is the dependent variable. In part a) it's the monthly electricity consumption and in part b) the monthly natural gas consumption. $i$ is the index of the residence and $t$ is time index. In our case the time is the month and year of the billing record. A further parameter is $code\_change_i$. It is an indicator variable and we already used this parameter in exercise 2.1. As a reminder, $code\_change_i$ is equal to $1$ if the residence has been constructed after the energy code change and $0$ otherwise. $X_i$ contains our explanatory variables. In the first part of this exercise it includes the variables `logfeet`, `centralair`, `shingled` and dummy variables for the variables `beds`, `baths`, `stories` and `zip`. In part 2 of this exercise we use different explanatory variables. You will see it later. $v_t$ is a "month-year specific intercept" (paper, p. 11). The last parameter is the error term $\epsilon_{it}$.
<br> To sum up, this model explains the differences in energy consumption by controlling for observable (housing) characteristics. Especially the $\textrm{zip-code dummies}$ "accounts for unobserved heterogeneity that is common among all residences within the same zip code" (paper, p. 14f.). Additionally $v_t$ "controls for month-to-month effects common to all residences" ( paper, p.11) like changes in the weather or in the energy prices. 

#< info "dummy variable"
We need dummy variables in a regression to represent explanatory variables when they are "qualitative in nature" ( Kennedy (2008), p. 232). You can define a dummy variable as "an artificial variable constructed such that it takes the value unity whenever the qualitative phenomenon it represents occurs, and zero otherwise" ( Kennedy (2008), p. 232).
<br> We have to be careful when using dummy variables in a regression model containing an intercept. To avoid "perfect multicollinearity" ( Kennedy (2008), p. 233) and be able to compute an OLS estimator we have to omit one of the dummy variables (according to Kennedy (2008), p. 232-234). Perfect multicollinearity "arises when one regressor is a perfect linear function of the other regressors" ( Stock and Watson (2007), p. 204). More information you get in Stock and Watson (2007), p. 203-208.

You can implement a dummy variable in R by using the command `factor()`. If a variable has already been transformed into a factor you can see the different levels of this factor by using the function `levels()`.  
```{r "3_1",optional=TRUE}
# load data 
library(foreign)
dat = read.dta("CCC_regression1.dta")

# create factor or dummy variable
Dbeds = factor(dat$beds)

# levels of dummy variable Dbeds
levels(Dbeds)
```
In this case there are four dummy variables combined in the variable `Dbeds`. For every value of the variable `bed` one dummy variable has been created. Let us denote them by $\textrm{Dbed2}$ to $\textrm{Dbed5}$. $\textrm{Dbed2}$ takes the value $1$ if the residence has $2$ bedrooms and $0$ otherwise. Similar for $\textrm{Dbed3}$, it is $1$ if the building has $3$ bedrooms and 0 otherwise and so on. 

*reference: https://stat.ethz.ch/R-manual/R-devel/library/base/html/factor.html, https://stat.ethz.ch/R-manual/R-devel/library/base/html/levels.html*
#>

We will estimate this model by using OLS estimation. We are mainly interested in the estimate of $\delta$ because it states the annual difference in the energy consumption between the pre- and post-code residences. In part a) it captures the electricity consumption and in part b) the consumption of natural gas. We expect estimates of $\delta$ being smaller than zero. This would indicate that the building code change leads to a reduced energy consumption. 

Now we want to implement this linear regression in R. There are several functions we could use to estimate linear models. You may have already used the function `lm()` for this purpose. But in this problem set we will use the function `felm()` out of the package `lfe` because our linear model has a lot of factors with many levels and additionally we are not interested in their estimates. Further information about the `lfe`-package you can get on https://cran.r-project.org/web/packages/lfe/lfe.pdf. And you find more details about the function `felm()` in the **info box** below. It also contains an example how to use this function to perform a linear regression in R. 

#< info "felm()"
In general, you can use the function `felm()` to run a linear regression. But it has additionally features in comparison to the function `lm()`. Beside the response variable you can specify 4 arguments of the `felm()`-function. They are written as a 4-part-formula separated by $|$. But in case of a simple linear regression `lm()` and `felm()` have the same arguments and work similar.    

The following code shows how to use the function in R. First load the package `lfe`. Then we choose a simple linear regression which is part of a regression we use in a later exercise. We want to regress `elec` on `code_change` and `logfeet`. Hence, `elec` is the dependent or response variable and the others are the explanatory variables which belong to the first part of the 4-part-formula. We write $elec \sim code\_change + logfeet$. At the end we have to specify the data frame. In our case it is called `dat`. 
```{r "3_1__2",optional=TRUE, eval=FALSE}
#load the package
library(lfe)

# perform a simple linear regression
felm(elec ~ code_change + logfeet, data=dat)
```

Now we will see one additional feature of the function `felm()`. In this problem set we will deal with regression models with many factors which have a lot of levels. Further we only use the factors for controlling the group effects and are not interested in their estimates. Hence, we use the function `felm()` which is able to project the factors out before it performs an OLS estimation. 
<br> We expand our linear model and add some factors named `controls`. We don't want to estimate them. We implement this model in R by writing $elec \sim code\_change + logfeet ~|~ controls$. Hence, the second part of the 4-part-formula consists of factors which we want to project out. 
```{r "3_1__3",optional=TRUE, eval=FALSE}
# perform a linear regression with projecting out the factors 
felm(elec ~ code_change + logfeet | controls, data=dat)
```

*reference: http://finzi.psych.upenn.edu/R/library/lfe/html/felm.html,  https://cran.r-project.org/web/packages/lfe/lfe.pdf*
#>

We have to be careful when we perform the regression with our data because error terms are correlated (already mentioned or explanation; "to account for potential serial correlation of the error terms")
Therefore we use standard errors that are clustered at the residence level. Take a look at the following **info box** if you like to know how to get clustered standard errors if you use function `felm()`.
(compare problems of regression)

#< info "clustered standard errors"
We have to specify another part of the 4-part-formula of the `felm()`-function to get clustered standard errors. In our case we use the variable `home_id` for clustering. But the cluster option is the fourth part of the formula. Hence, we have to insert $0$ in the third part. This part contains the instrumental variables and in this problem set we don't need them. So the third part is always $0$. More information about instrumental variables you get in Stock and Watson (2007), p. 421ff. (chapter 12).  

```{r "3_1__4",optional=TRUE, eval=FALSE}
# regression with robust standard errors which are clustered at the residence level
felm(elec ~ code_change + logfeet | controls | 0 | home_id, data = dat)
```

*reference: http://finzi.psych.upenn.edu/R/library/lfe/html/felm.html, https://cran.r-project.org/web/packages/lfe/lfe.pdf*
#>
 
### 1. Regressions

Before we can estimate the parameters of our regression model we have to load the data. As you already loaded several Stata data frames into R you only have to press `check` but first click on the button `edit`. The required data frame for this exercise is called `CCC_regression1.dta`.

```{r "3_1__5"}
#< task
# load the package foreign
library(foreign)

# read data set CCC_regression1.dta into R and assign it to dat
dat = read.dta("CCC_regression1.dta")
#>
```

We will use the function `felm()` for our regression analysis. So first of all we have to load the package `lfe`. Enter the correct command for loading the package in the following code chunk. Then press `check`. 

```{r "3_1__6"}
#< task
# load the package lfe
# enter your code here...
#>
library(lfe)
```

We will perform our regression analysis in two steps. First we look at the electricity consumption and in part b) we consider the natural gas consumption.

#### a) Electricity consumption

Now we will perform the first regression analysis of this problem set. We want to calculate the estimates for the annual differences in electricity consumption between the residences built before and built after the code change. You can find the equation of the **regression model 1** at the beginning of this exercise. You may take a look at this formula again. 
<br> On the basis of this model we will set up the regression formula by using the specific variables of our data frame. We regress `code_change` on `elec` but we include $8$ additional variables which also affect our dependent variable `elec` to get an precise and unbiased OLS estimator. We estimate the effect of the regressor `code_change` on the electricity consumption by holding these other $8$ variables constant. Hence, we can refer to these variables as control variables (according to Stock and Watson (2007), p. 186, 193f.). The first $3$ control variables are `logfeet`, `centralair` and `shingled`. The other $5$ variables - `baths`, `beds`, `stories`, `zip` and `monthyear` - have to be transformed into dummy variables by using the function `factor()`. We are not interested in the estimates for the dummy variables so we can project them out. Hence the regression formula is 
$$elec \sim code\_change + logfeet + centralair + shingled ~|~ factor(baths) + factor(beds) + factor(stories) + factor(zip) + factor(monthyear).$$ 
We additionally use clustered standard errors. Hence, we have to add the variable `home_id` as the fourth part in the formula of the function `felm()`. But we don't need instrumental variables so the third part contains $0$ as an argument.
<br> Run the regression and save the results in the variable `reg1a`. Then show the results with the command `summary()`. More information about this function you can get on https://www.rdocumentation.org/packages/lfe/versions/1.5-1045/topics/summary.felm.

The code is already given. Just press `check`. 
```{r "3_1__7"}
#< task
reg1a = felm(elec ~ code_change + logfeet + centralair + shingled | factor(baths) + 
             factor(beds) + factor(stories) + factor(zip) + factor(monthyear) | 0 | 
             home_id, data=dat)

# output of the results
summary(reg1a)
#>
```

Here you can see the results of our first linear regression. Now we want to interpret these results. Before we take a look at the estimated coefficients we take into account the **coefficient of determination** $R^2$ (**R-squared**). We are mainly interested in the value of the adjusted and multiple R^2 of the full model. Both coefficients are very similar to each other and roughly take the value of $0.499$. This means the explanatory variables explain approximately $50\%$ of the variance of the dependent variable, which is the electricity consumption in this model (according to Stock and Watson (2007), p. 125). To sum up, we can say the "model fits the data well"( paper, p. 12) . More information about the R^2 you can find in the **info box** below.

#< info "(adjusted) coefficient of determination"
When estimating the coefficients of a regression you're not only interested in the resulting estimates but also in the goodness of fit, that is how well the data is predicted by your estimates. Visually, how well the regression line fits the data. 

The R^2 can take values between $0$ and $1$. Whereas these two values are not typically attained. A value of R^2 close to $1$ means "the regressors produce good predictions of the dependent variable $Y_t$ in this sample, in the sense that the variance of the OLS residual is small compared to the variance of the dependent variable $Y_t$. If the R^2 is nearly $0$ the opposite is true" ( Stock and Watson (2007), p. 238).
<br> The R^2 can be mathematically defined in two different ways. But first of all let us specify some notations: the different observations are denoted by $t=1,...,T$, $\hat{Y}_t$ are the predicted values of the dependent variable $Y_t$ and $\bar{Y}$ is the average of $Y_t$.
<br>First definition: $$R^2 = \frac{ESS}{TSS} = \frac{\sum^T_{t=1}(\hat{Y}_t-\bar{Y})^2}{\sum^T_{t=1}(Y_t-\bar{Y})^2}$$ with $\textrm{ESS}$ being the explained sum of squares and $\textrm{TSS}$ the total sum of squares (according to Stock and Watson (2007), p. 123f.). In words, R^2 is the "fraction of the sample variance of $Y_t$ explained by the regressors" ( Stock and Watson (2007), p. 200).
<br>Second definition: $$R^2 = 1 - \frac{SSR}{TSS} = 1 - \frac{\sum^T_{t=1}\hat{\epsilon}^2_t}{\sum^T_{t=1}(Y_t-\bar{Y})^2}$$ with $\textrm{SSR}$ being the sum of squared residuals (according to Stock and Watson (2007), p. 123f.). This means verbally, R^2 is the "fraction of the variance of $Y_t$ **not** explained by the regressors" ( Stock and Watson (2007), p. 200).

The problem with R^2 in multiple regression is that its value increases as soon as a new explanatory variable, with an estimated coefficient different to $0$ (which is typical), is added to the regression model. Hence, we use the adjusted R^2 to avoid this problem. The adjusted R^2 is always smaller than R^2 and can also be negative. Mathematically you have to correct R^2 by the factor $\frac{T-1}{T-K-1}$ to get a formula for the adjusted R^2, where $K$ is the number of different coefficients $\beta_i$. The exact formula of the adjusted R^2 is: $$adj.~R^2 = 1 - \frac{T-1}{T-K-1} \cdot \frac{SSR}{TSS}.$$ More details you can find in Stock and Watson (2007), p. 201.

But do not only rely on this factor when checking for a good regression model. 

*reference: Kennedy (2008), p. 13f., 26-28 and 79f., Stock and Watson (2007), p. 123f., 200-202, 237f.*
#>

Then we take a look at the different coefficient estimates along with their significance niveaus. Two of the estimates are significant which is indicated by the stars. One of the coefficients is the estimate for $\delta$ which we denote by $\hat\delta$. It is the coefficient of the variable `code_change` and hence of our mainly interest. To be more precise, $\hat\delta = -47.62$ at a $5\%$ significance level because of the two stars. In case of a correct interpretation we have to consider the sign and the value of the estimate. This estimate indicates that post-code residences consume roughly $47.6$ kWh per month less electricity than pre-code buildings. 
<br>Generally, we can interpret the regression coefficient $\beta_1$ (or similar for the OLS estimate) in a simple regression model like $Y = \beta_0 + \beta_1 \cdot X_1 + \epsilon$ in the following way:
<br>- an increase of $X_1$ by one unit is associated with an **increase** of $Y$ by $\beta_1$ units, if $\beta_1$ is **positive** and
<br>- an increase of $X_1$ by one unit is associated with an **decrease** of $Y$ by $\beta_1$ units, if $\beta_1$ is **negative** 
<br>(according to Stock and Watson (2007), p. 120f.). 
When we expand the simple regression model to a multiple regression model we only have to add a supplement to the interpretation which says "holding the other regressors constant" (according to Stock and Watson (2007), p. 193f.). 
<br> The interpretation of our previous estimate $\hat\delta$ is different because the variable `code_change` is a dummy variable and not a continuous variable. Hence, we cannot refer $\hat\delta$ as a slope. Instead $\hat\delta$ describes the difference in the sample means of `elec` for the two groups (according to Stock and Watson (2007), p. 158f.). In our data set we can interpret $\hat\delta$ as the difference in the electricity consumption of pre- and post-code residences.

We can convert our estimate $\hat\delta = -47.62$ into percentage difference. Hence, we get a result of approximately $4\%$ monthly decrease in the electricity consumption of the Gainesville households caused by the building code change.

#< info "exemplarily calculation of percentage difference"
This info box shows an exemplarily calculation of percentage difference. We will transform our estimate $\hat\delta$ into percentage difference. In this example, $\hat\delta$ is the coefficient estimate of the variable `code_change` and the variable `elec` is the dependent variable of the regression model.  

You can see the calculation in the following code chunk. We have to divide `delta_hat` by `mean_elec` and then multiply this value by $100\%$. A positive value indicates an increase and on the opposite, a negative value means a decrease. 
```{r "3_1__8",optional=TRUE}
delta_hat = -47.62
mean_elec = 1146.4

# estimate transformed into percentage difference
delta_pd = delta_hat/mean_elec * 100#%
delta_pd
```
You see the result which is roughly $4\%$. It coincides with the value above the info box.  
#>

The other significant OLS estimate is the coefficient for `logfeet`. This estimate is positive and highly significant at a level of $1\%$ (according to Stock and Watson (2007), p. 79f.). This means in general that "larger residences consume more electricity" (paper, p.12). 
<br> Now we want interpret this coefficient estimate in more detail. We cannot use the interpretation rule from above because in this case the explanatory variable is in **logarithms**. Hence, we state a new rule. Let us assume a simple regression model of the form $Y = \beta_0 + \beta_1 \cdot ln(X_1) + \epsilon$. 
<br> Generally, we can interpret the regression coefficient $\beta_1$ (or similar for the OLS estimate) in the following way:
<br>- an increase of $X_1$ by **one percent** is associated with an **increase** of $Y$ by $0.01 \cdot \beta_1$ units, if $\beta_1$ is **positive** and
<br>- an increase of $X_1$ by **one percent** is associated with an **decrease** of $Y$ by $0.01 \cdot \beta_1$ units, if $\beta_1$ is **negative**
<br>(according to Stock and Watson (2007), p. 269f. and 273).

Let's come back to our data and regression results. The coefficient estimate for the variable `logfeet` takes the value $958.940$. This means, if the square footage of a building increases by $1\%$ (then) the electricity consumption of this household will increase by roughly $9.6$ kWh per month. We can also consider a $10\%$ increase in the square footage. Then the electricity consumption of this household would increase by roughly $96$ kWh per month or by $8.3\%$ (according to paper, p. 12). 

#### b) Natural gas consumption

Now we will run the next regression to determine the estimates for the annual differences in natural gas consumption between the pre- and post-code residences. There is only one change in the regression formula, namely the depend variable is now `gas` instead of `elec`. Press `check` to perform the regression. The results will be saved in the variable `reg1b`.  

```{r "3_1__9"}
#< task
reg1b = felm(gas ~ code_change + logfeet + centralair + shingled | factor(baths) +
             factor(beds) + factor(stories) + factor(zip) + factor(monthyear) | 0 | home_id, 
             data=dat)
#>
```

This time we don't show the regression results with help of the function `summary()`. Instead we will show the results of both regressions `reg1a` and `reg1b` next to each other. For this purpose we use the function `screenreg()` out of the package `texreg`. First we have to load this package. Then we use the function `screenreg()` with the following arguments: `list(regression1, regression2,...)` is used to specify the regression models. The command `custum.model.names` gives a different name to the regression models instead of Model 1 and Model 2. The arguments `omit.coef` and `digits` are explained directly in the code box. An important command is `stars = c(0.01, 0.05, 0.10)`. It changes the significance levels. In our case it will print $\textrm{***}$ behind the values of the coefficients where the p-value is below $0.01$, $\textrm{**}$ when the p-value is below $0.05$ and $\textrm{*}$ if the p-value is lower than $0.1$. You can find more information about the package `texreg` and its functions on https://cran.r-project.org/web/packages/texreg/texreg.pdf.

Click on the button `check` to show the resulting table.
```{r "3_1__10"}
#< task
# load the package
library(texreg)

# output regression results
screenreg(list(reg1a, reg1b), custom.model.names = c("Electricity","Natural gas"),
          # output without intercept
          omit.coef = "(Intercept)", 
          # round numbers to 3 decimal places
          digits = 3, 
          # define different significance niveau (use the one the authors used in paper)
          stars = c(0.01, 0.05, 0.10))
#>
```

This table reports the results of our two regressions of **model 1**. The clustered standard errors are written in brackets underneath the estimates. When we compare both regressions we can see that the respective estimates of both regressions have the same signs. We are mainly interested in the estimates of the variable `code_change` and these estimates are in both regression models negative. You get a detailed interpretation when solving the next quiz. 
<br> We will now take a closer look at the results of the regression `reg1b` for natural gas consumption. The R^2 and the adjusted R^2 are again equal and take the value $0.452$. This value is a little bit lower compared to our first regression but it also shows that model fits the data well (paper p.12). In this case the explanatory variables explain roughly $45\%$ of the variance of the natural gas consumption (according to Stock and Watson (2007), p. 125). 
<br>All four estimates are statistically significant but the significance niveaus for the estimates of `centralair` and `shingled` are really low and hence not reliable (according to paper p.13). So we focus on the coefficient estimates for the variables `code_change` and `logfeet`.       

Answer the question of the following two quizzes. 

#< quiz "regression results - code_change"
question: Denote the correct statement.
sc:
    - Post-code residences consume roughly 1.5 therms per month more natural gas than pre-code buildings. This result is significant at the 5 % niveau.
    - Post-code residences consume roughly 1.5 therms per month less natural gas than pre-code buildings. This result is significant at the 5 % niveau.*
success: Great, your answer is correct!
failure: Try again.
#>

<br> To sum up, the coefficient estimates for the variable `code_change` in `reg1a` and `reg1b` show that the building code change leads to a reduction in electricity and natural gas consumption. To be more exact, the code change causes an approximately $4\%$ monthly decrease in the electricity consumption and $1.5$ therms per month in the gas consumption. Translated into percentage difference, means a reduction of $6.4\%$ natural gas consumption per month. If you don't know how to transform the estimate into percentage difference look at the **info box** in part a). 

#< quiz "regression results -  logfeet"
question: Complete the following sentence. If the square footage of a residence increases by 10 percent, then the natural gas consumption will increase by ... therms per month. Round the number to 2 decimal places.
answer: 2.98
roundto: 0.01
#>

#< award "regression master level 1"
Great! You are able to interpret regression results correctly! 
#>

### 2. Robustness check

In this section we perform a robustness check of both regressions of the previous part. A robustness check is a test "where the researcher examines how certain "core" regression coefficient estimates behave when the regression specification is modified by adding or removing regressors" (Lu and White (2013), p. 194). Core variables are those regressors which are of mainly interest for us. "If the coefficients are plausible and robust, this is commonly interpreted as evidence of structural validity" (Lu and White (2013), p. 194f.).

As reminder, the formula of the regression model of the first section of this exercise is $$Y_{it} = \delta \cdot code\_change_i + \beta \cdot X_i + v_t + \epsilon_{it}.$$

Now we vary this model and allow "time effects to differ by each zip code" (according to paper, p.12). This means we no longer use dummy variables for `monthyear`(which is represented by $v_t$) and `zip`(which is one element of $X_i$) separately but "zip-code-by-month-year" dummy variables ( paper, p.12). We get such dummy variables by an interaction of the time effect $v_t$ and each of the dummy variables of the variable `zip`. To sum up, we no longer use "uniform time" effects but test for "zip-code specific time effects" (according to paper, p.13 and 15). 

#< info "interaction of factors"
We want to compute the interaction of two factors in R. As described above we allow the time variable `monthyear` to differ by the variable `zip`. The two variables are part of the data frame `dat` so we have to write `dat$zip`. Later in the regression formula we can leave out the first part and only write `zip` because we specify the data frame separately at the end of the formula. The R command to compute interactions is simply called `interaction()`. You can abbreviate this command by just writing `:` between the two factors which should be interacted.  
```{r "3_1__11",optional=TRUE, eval=FALSE}
# long version
interaction(factor(dat$zip), factor(dat$monthyear))

# short form
factor(dat$zip):factor(dat$monthyear)

```

You can convince yourself of a correct-implemented interaction by showing the different levels of the factors. First show the levels of the individual factors and at the end the length of the interacted one and it should be the product of the both individual factors. In our case $9*36=324$ levels.
```{r "3_1__12",optional=TRUE}
# load data frame
dat = read.dta("CCC_regression1.dta")

# individual factors
levels(factor(dat$zip))
levels(factor(dat$monthyear))

# interacted factor
length(levels(factor(dat$zip):factor(dat$monthyear)))
```

*reference: https://stat.ethz.ch/R-manual/R-devel/library/base/html/interaction.html, https://cran.r-project.org/web/packages/lfe/lfe.pdf, http://finzi.psych.upenn.edu/R/library/lfe/html/felm.html*
#>

Similar to the first part of this exercise we first calculate the estimates for the electricity consumption and then in part b) we run a second regression to get the estimates for the natural gas consumption.

#### a) Electricity consumption

Now we will test this alternative specification and run a regression with the function `felm()`. The methods and arguments of the function are similar to the ones of the previous section of this exercise, besides one exception. We now use the interacted dummy variables of `zip` and `monthyear`. In the **info box** above you can see how to implement the interaction in R. 
<br> First we regress on the electricity consumption. This means, our dependent variable is `elec`. We save the regression results in the variable `reg2a`. Press `check` to run the regression.  

```{r "3_1__13"}
#< task
# run the regression               
reg2a = felm(elec ~ code_change + logfeet + centralair + shingled  | factor(baths) +
             factor(beds) + factor(stories) + factor(zip):factor(monthyear) | 0 | home_id, 
             data=dat)
#>
```

Let us show the results of the regression. We don't want to consider the results of this regression isolated but in comparison with the results of the originally regression without additional specification. This regression was named `reg1a`. Hence, we show the results of `reg1a` and `reg2a` with help of the function `screenreg()`. We us the same arguments of the function as we did in the previous exercise. Click on the button `check` to show the results. 

```{r "3_1__14"}
#< task
# output of regression results 
screenreg(list(reg1a, reg2a), 
          custom.model.names = c("Electricity (1.)","Electricity (2.)"),
          omit.coef = "(Intercept)", digits = 3, stars = c(0.01, 0.05, 0.10))
#>
```

We find very similar results for both regressions. The OLS estimates all have the same signs and the estimated values are really close to each other. Additionally the significance levels are the same and the differences between the standard errors are small. 
<br> Comparing the adjusted R^2 of both regressions we see that the one of the second regression is a little bit greater. It takes the value $0.511$ instead of $0.499$. All in all, both models fit the data well and but in regression `reg2a` a greater part of the variation in the electricity consumption is explained (according to Stock and Watson (2007), p. 202).
<br> To sum up, the coefficient estimates seem robust towards a variation of the regression model.  

#### b) Natural gas consumption

Now we perform a robustness check for the natural gas consumption. Hence, the response variable of our regression is `gas`. All other parts of the regression formula stay the same.

**Task:** Estimate the annual differences in the natural gas consumption between the pre- and post-code residences. Use the regression model with the additional specification of "zip-code-specific-time-effects" and the same methods as in the previous regressions  - like OLS estimation with standard errors clustered at the residence level and let the factors be projected out. Enter your code and press `check` afterwards. If you need an additional advice press `hint`. 

```{r "3_1__15"}
#< task
# enter your code here...
# reg2b = ???
#>
reg2b = felm(gas ~ code_change + logfeet + centralair + shingled | factor(baths) +
             factor(beds) + factor(stories) + factor(zip):factor(monthyear) | 0 | home_id, 
             data=dat)

#< hint
display("Your code should look like: reg2b = felm(??? ~ code_change + logfeet + centralair + shingled | factor(baths) + factor(beds) + factor(stories) + factor(zip):factor(monthyear) | 0 | home_id, data=dat). Replace the ??? by the correct dependent variable.")
#>
```

#< award "regression master level 2"
Great! You have performed a regression completely on your own. Even with an interaction of two factors. 
#>

You estimated the coefficients of the expanded regression model. The results are stored in the variable `reg2b`. Now we want to compare this model with our original model with uniform time effects (`reg1b`) and check for robustness. Hence, we use the function `screenreg()` to show both regression results next to each other.
```{r "3_1__16"}
#< task
# show regression results 
screenreg(list(reg1b, reg2b), custom.model.names=c("Natural gas (1.)","Natural gas (2.)"),
          omit.coef="(Intercept)", digits=3, stars = c(0.01, 0.05, 0.10))
#>
```

This time we interpret the results by solving a quiz.

#< quiz "robustness check - natural gas"
question: Denote the correct statement.
sc:
    - Coefficient estimates of both regression models are very similar.*  
    - The two regressions produce really different estimates.  
success: Great, your answer is correct!
failure: Try again.
#>

<br> To sum up, both regression models for the annual differences in natural gas consumption between the pre- and post-code buildings fit the data well. It also holds for the results for electricity consumption (part 2 a) of this exercise). All in all the resulting estimates are stable between the corresponding two regression models, that are `reg1a` and `reg2a` for electricity and `reg1b` and `reg2b` for natural gas consumption. The differences of the regression models 1 and 2 are the time effects. We used uniform month-year effects in model 1 and allowed a variation by the variable `zip` in model 2.  
<br> In conclusion, regression model 2 "is a useful robustness check because it accounts, for some extent, for spatial differences in the time trends that might be correlated with areas of predominantly newer or older constructions" (paper, p. 15).

*This exercise refers to the page 11 to 13 and table 3 of the paper.*

## Exercise 3.2 -- Monthly differences

In the previous exercise we estimated the annual differences of the pre- and post-code residences. Our main findings were that the code change led to a reduction in the residential energy consumption.  
<br>In this exercise we no longer want estimates which are averaged across the months but we calculate individual estimates for each month of the year. The reason for this change is that the effects of the code change probably differ substantially between the different months within one year because of the variability of the weather. And hence, the averaging procedure of the previous exercise may have produced imprecise estimates.    
The variability of the weather throughout the year is really high. Therefore, the demand for cooling and heating also differs heavily among the months. And because of the fact that natural gas is used for cooling and electricity for heating there is a dependency between the weather variability and residential energy consumption. To conclude, it is necessary to estimate monthly differences of the pre- and post-code residences (to get better/more precise estimates).
<br> Hence, we expand the regression model of the previous exercise by dummy variables for the different months of the year. We assign the new regression to **model 2** and this model looks like $$Y_{it} = \delta \cdot code\_change_i \times month_t + \beta \cdot X_i + v_t + \epsilon_{it}.$$ All parameters except of one are the same as in our regression **model 1** of exercise 3.1. The new parameter is called $month_t$ and its a "vector of dummy variables for each of the $12$ months in the calendar year" ( paper, p. 13). The variables `code_change` and `month` are interacted. So we get $12$ different estimates $\hat\delta$ for the effect of the code change on energy consumption. Each estimate is associated with one month of the year.
<br> We still use the same estimation methods like OLS estimation and standard errors clustered by the variable `home_id`. 

In the first part of this exercise we run two regressions of **model 2**. We first use the electricity consumption as dependent variable and then in part b) the natural gas consumption. As results we get the estimates of the monthly differences in the energy consumption of the pre- and post-code residences. In part 2 of this exercise we visualise the results of part 1 by two figures. 

Let's start with our analysis and load the data. The data frame for this exercise is called `CCC_regression2.dta`. There is only one difference to the data frame of the previous exercise - this one contains an additional column which is called `month`. First press `edit` and afterwards `check` to load the data frame. 
```{r "3_2"}
#< task
# load the package
library(foreign)

# read data set CCC_regression1.dta into R and assign it to dat
dat = read.dta("CCC_regression2.dta")
#>
```

### 1. Regressions

**Task:** Load the required package to be able to perform a regression with the `felm()`-function. Enter your code and then click `check`.  
```{r "3_2__2"}
#< task
# load the package
# enter your coder here...
#>
library(lfe)

#< hint
display("The function felm() is part of the lfe package.")
#>
```

Before we run the first regression of **model 2** we take a closer look at the "code-change-by-month" dummy variables. The R code for implementing this interaction is $code\_change:factor(month)$. The interaction term contains $12$ different variables which are again dummy variables. Let us call the first one `CodeMonth1`. This variable is $1$ if $code\_change == 1$ and $monthD1 == 1$ (means: January) and $0$ otherwise. Let the second variable be `CodeMonth2` which takes the value $1$ if $code\_change == 1$ and $monthD2 == 2$ (means: February) and so on.    

We will perform our regression analysis in two steps. Similarly to the previous exercise. Firstly, we look at the electricity consumption and in part b) we consider the natural gas consumption.

#### a) Electricity consumption

Now let's run the first regression of **model 2**. You can find the equation of this model at the beginning of this exercise. We want to calculate the estimates for the monthly differences in electricity consumption between the pre- and post-code residences. Hence, we now use the dependent variable `elec`.

Run the regression and save the results in the variable `reg1a_month`. We need this variable later in section 2. for plotting. Then show the results of the regression with the function `screenreg()` out of the package `texreg`. We are only interested in estimates of the monthly code change effect so we omit all other coefficients in the output table.  
<br> Press `check` to run the regression and show the results.  

```{r "3_2__3"}
#< task
reg1a_month = felm(elec ~ code_change:factor(month) + logfeet + centralair + shingled | 
                     factor(baths) + factor(beds) + factor(stories) + factor(zip) + 
                     factor(monthyear) | 0 | home_id, data=dat)

# load the package
library(texreg)

# show the regression results 
screenreg(reg1a_month, custom.model.names = c("Electricity"), 
          # omit all coefficients apart from the monthly code change variables
          omit.coef = "(Intercept)|(logfeet)|(centralair)|(shingled)", 
          digits = 3, stars = c(0.01, 0.05, 0.10))
#>
```

We shortly interpret the results of `reg1a_month`. You find a detailed interpretation and further explanations of the results at the end of section 2.
<br> You see that $R^2$ and $\textrm{adj.} ~R^2$ are approximately $0.5$ which means the regressors predict roughly $50\%$ of the variance of the monthly electricity consumption (according to Stock and Watson (2007), p. 125).
<br> $7$ coefficient estimates are significant at a level of $10\%$ or even smaller. These estimates correspond to the months April to October and are all negative. Meaning that post-code residences consume less electricity in the warmer months of the year than pre-code residences. The greatest difference between the electricity consumption of pre-code and post-code residences is in June - where after-code-change buildings consume roughly $114.3$ kWh (per month) less electricity than residences which were built before the code change.

#### b) Natural gas consumption

Now we turn to the estimation of the monthly effect of the code change on residential natural gas consumption. Again we use **regression model 2** but in this case the dependent variable is `gas`. 
  
**Task:** Estimate the monthly differences in natural gas consumption between the pre- and post-code residences by OLS estimation with the function `felm()`. Use model 2 and the same methods as in the previous exercises (e.g. project out factors, standard errors clustered by `home_id`). Store the results in the variable `reg1b_month`. Enter your code and then press `check`. 

```{r "3_2__4"}
#< task
# enter your code here...
# reg1b_month = ???
#>
reg1b_month = felm(gas ~ code_change:factor(month) + logfeet + centralair + shingled | 
                     factor(baths) + factor(beds) + factor(stories) + factor(zip) + 
                     factor(monthyear) | 0 | home_id, data=dat)

#< hint
display("Your command should look like reg1b_month = felm(??? ~ ??? : ??? + logfeet + centralair + shingled | factor(baths) + factor(beds) + factor(stories) + factor(zip) + factor(monthyear) | 0 | home_id, data=dat). Replace the ??? by the response variable and two explanatory variables.")
#>
```

**Task:** Show the regression results. Don't use the function `summary()`. The necessary package is already loaded. Some arguments of the function are given. Replace the ??? by the correct commands. Then uncomment the lines and press `check`. 
```{r "3_2__5"}
#< task
# Replace the ??? by the correct commands.
# ???(???, custom.model.names=c("Natural gas"),
# omit.coef="(Intercept)|(logfeet)|(centralair)|(shingled)", digits = 3, 
# stars = c(0.01, 0.05, 0.10))
#>
screenreg(reg1b_month, custom.model.names = c("Natural gas"), 
          omit.coef = "(Intercept)|(logfeet)|(centralair)|(shingled)", 
          digits = 3, stars = c(0.01, 0.05, 0.10))

#< hint
display("The function you should use is called screenreg and the regression model reg1b_month.")
#>
```

#< award "regression master level 3" 
Great! You can run a regression, show its results and interpret them. 
#>

You find three highly significant coefficient estimates at a $1\%$-level. They belong to the months January, February and December. These estimates are again all negative and hence show a reduced monthly natural gas consumption of post-code residences in comparison to pre-code buildings. The lower gas consumption ranges between $7.3$ and $9.3$ therms per month.   
<br> The coefficient of determination indicates a good model fit.

#< quiz "interpretation R-squared"
question:  Complete the following sentence. The regressors explain roughly ... % of the variance of the monthly natural gas consumption. 
answer: 46
roundto: 1
#>

<br> You find a detailed interpretation of these regression results at the end of the following part of this exercise. 

### 2. Visualisation

Now we will visualise the main results of the regressions `reg1a_month` and `reg1b_month` with two figures. We will draw two graphs which will show the estimates for the code change effect for each month, i.e. the $\hat\delta$ of **regression model 2**. Due to a better comparison we transform the $\hat\delta$ into percentage difference. Thus, we report the $\hat\delta$ "as the percentage change from average consumption for that particular month" ( paper, p. 13). Additionally we will draw the $95\%$ confidence intervals for the estimates because of sampling uncertainty (according to Stock and Watson (2007), p. 155f.).

We start with a preparation of the data frame and calculate the average monthly electricity and natural gas consumption. We need these mean values for transforming the estimates into percentage differences.
 
```{r "3_2__6"}
#< task
# load the package
library(dplyr)

# prepare data set and calculate the mean values for each month
dat_month = dat %>%
  select(elec,gas,month) %>%
  group_by(month) %>%
  summarise(Melec = mean(elec), Mgas = mean(gas))
#>
```

Hence, the mean values are stored in the variables `Melec` and `Mgas` for electricity and natural gas consumption, respectively.
<br> In the next step we select the estimates $\hat\delta$ of the regressions `reg1a_month` and `reg1b_month`. The first regression yields the electricity results and the second one the results for natural gas consumption. You get the estimates by using the command `reg_name$coef`. Store them in the vectors `deltaElec` and `deltaGas`.  
<br> Then transform the estimates into percentage differences. If you can't remember how to do this take a look at the **info box** in exercise 3.1 part 1.  

```{r "3_2__7"}
#< task
# store estimates in separate vectors
deltaElec = reg1a_month$coef[4:15] 
deltaGas = reg1b_month$coef[4:15] 

# transform deltaElec into percentage differences (pd)
pd_Elec= deltaElec/dat_month$Melec * 100#%
#pd_Elec

# transform deltaGas into percentage differences
pd_Gas= deltaGas/dat_month$Mgas * 100#%
#pd_Gas
#>
```

Now we will calculate the confidence intervals. The R function `confint()` computes them for the parameters of a fitted model. We will use $95\%$ confidence intervals so we have to choose `level=0.95`. More details about the function `confint()` you can find on https://stat.ethz.ch/R-manual/R-devel/library/stats/html/confint.html. To get the confidence intervals for $\hat\delta$ we have to select particular rows. In our case from row $4$ to $15$. If you like to get more information about the confidence intervals look at the **info box**. 

#< info "confidence intervals"
A confidence interval for a parameter $\beta$ is an interval which contains the true value of this parameter with a pre-defined probability (according to Stock and Watson (2007), p. 776). Hence, a $95\%$ confidence interval for the regressor $\beta_1$ contains the true value of $\beta_1$ with a probability of $95\%$.A second, but equivalent definition is the following: we cannot reject this set of values when using a two-sided hypothesis test with a significance level of $5\%$.  

We can construct confidence intervals for a single regressor by using its OLS estimate and its standard error (SE). The sample size of our data is large so we can assume a (asymptotic) standard normal distribution and therefore the critical value of a two-sided test statistic is $\pm1.96$ (according to Stock and Watson (2007), p. 78f.). Hence, the $95\%$ confidence interval for the regressor $\beta_1$ is $$[\hat\beta_1 - 1.96 \cdot SE(\hat\beta_1), \hat\beta_1 + 1.96 \cdot SE(\hat\beta_1)]$$ (according to Stock and Watson (2007), p. 155-157). Similarly, you can construct a confidence interval for a single coefficient in a multiple regression model (according to Stock and Watson (2007), p. 223). 

*reference: Stock and Watson (2007), p. 78f., 155-157, 223 and 776* 
#>

We also have to transform the confidence intervals into percentage differences to make them suitable for drawing the figures.

```{r "3_2__8"}
#< task
# compute confidence intervals
CI_Elec = confint(reg1a_month, level=0.95)
CI_E_low = CI_Elec[4:15,1]
CI_E_up = CI_Elec[4:15,2]

CI_Gas = confint(reg1b_month, level=0.95)
CI_G_low = CI_Gas[4:15,1]
CI_G_up = CI_Gas[4:15,2]

# transform confindence intervals into percentage differences
CI_pdE_low = CI_E_low/dat_month$Melec * 100#% 
CI_pdE_up = CI_E_up/dat_month$Melec * 100#%

CI_pdG_low = CI_G_low/dat_month$Mgas * 100#%
CI_pdG_up = CI_G_up/dat_month$Mgas * 100#%
#>
```

In the following code box we create a data frame with the necessary variables or columns which we need later for drawing the figures. First, we generate a data frame with the two columns `pd_Elec` and `pd_Gas` of our estimates (measured as percentage differences). The function `cbind()` combines the two vectors by columns. Look at http://stat.ethz.ch/R-manual/R-devel/library/base/html/cbind.html for further information. Second, we generate a time column `date` which denotes the months of the year. Last, we add $4$ columns representing the upper and lower limits of the confidence intervals (also in percentage differences). Press `check` to create the data frame `dat_plot`. If you click on the button `data` the **Data Explorer** opens and shows the data frame.

```{r "3_2__9"}
#< task
# create a data frame with necessary variables for plotting 
dat_plot = as.data.frame(cbind(pd_Elec,pd_Gas)) %>%
  # add time column
  mutate(date=seq(from=as.Date("2002-01-01"), to=as.Date("2002-12-01"), by="month")) %>%
  # add columns with upper and lower borders of confidence intervals
  mutate(CI_pdE_low, CI_pdE_up, CI_pdG_low, CI_pdG_up)
#>
```

Now all preparations are done and we can plot the figures. The first figure illustrates the monthly effect of the code change on electricity consumption and the second graph shows the results for the natural gas consumption. 
<br> The code contains a lot of functions. I will only shortly explain the commands we never used before in this problem set. The function `ggtitle("...")` adds a title to the figure. The command `theme_bw()` creates a white background of the plot with black grid lines. Another new function is `geom_hline()`. To add a horizontal line at $y=0$ we use this function with the argument `aes(yintercept=0)`. You find further information on http://docs.ggplot2.org/0.9.3.1/geom_hline.html. 
We can specify the line type with the command `lty`. `lty=2` creates a dashed line. Last we add the confidence intervals by using the function `geom_errorbar()`. With the argument `aes(ymin=..., ymax=...)` we specify the lower and upper borders. You can read more about this function on http://docs.ggplot2.org/0.9.3.1/geom_errorbar.html. 

*further references: https://www.rstudio.com/wp-content/uploads/2015/06/ggplot2-german.pdf, http://www.statmethods.net/input/dates.html, http://docs.ggplot2.org/0.9.3/xylim.html, http://docs.ggplot2.org/current/scale_discrete.html*

```{r "3_2__10",fig.height=10, fig.width=9}
#< task
# load packages 
library(ggplot2)
library(scales)

# English description of figures
Sys.setlocale("LC_ALL", "English")

# figure for electricity consumption
e = ggplot(data=dat_plot, aes(x=date)) + xlab("") + ylab("percent difference") + 
  ggtitle("electricity consumption") + ylim(-14,8) + theme_bw() 

e1 = e + geom_point(aes(y=pd_Elec),size=4,col="red") + 
  geom_line(aes(y=pd_Elec),size=1,col="red") +
  scale_x_date(labels=date_format("%b"),breaks = date_breaks("1 month"))

e2 = e1 + geom_hline(aes(yintercept=0), lty=2, size=1)

e3 = e2 + geom_errorbar(aes(ymin=CI_pdE_low, ymax=CI_pdE_up),lty=2)

# figure for natural gas results
g = ggplot(data=dat_plot, aes(x=date)) + xlab("") + ylab("percent difference") + 
  ggtitle("natural gas consumption") + ylim(-35,25) + theme_bw() 

g1 = g + geom_point(aes(y=pd_Gas),size=4,col="red") + 
  geom_line(aes(y=pd_Gas),size=1,col="red") + 
  scale_x_date(labels=date_format("%b"),breaks = date_breaks("1 month"))

g2 = g1 + geom_hline(aes(yintercept=0), lty=2, size=1)

g3 = g2 + geom_errorbar(aes(ymin=CI_pdG_low, ymax=CI_pdG_up),lty=2)

# draw figures 
library(gridExtra)

grid.arrange(e3,g3)
#>
```

Now you can see the figures which illustrate the monthly effects of the code change on residential electricity and natural gas consumption. The estimates are reported as percentage difference from the average consumption of the corresponding month. Additionally the figures show the $95\%$ confidence intervals of the estimates. Now we will describe and interpret these figures. First we take a look at the electricity results and then at the results of the natural gas consumption. 
<br> Overall you find that post-code buildings consume less electricity during the warmer months than pre-code buildings. In the winter months all residences consume roughly the same amount of electricity. When we additionally consult the output of our regression analysis in part 1 we see that the coefficient estimates for the warmer months - from April to October - are significant at a $10\%$-level or even at a more stringent level. To sum up the results, we find that the residences built after the code change consume between $4.7\%$ and $8\%$ less electricity in the warmer months in comparison to the pre-code buildings. In contrast, the results for the colder months are not significant. But this fact doesn't cause problems here. The reason for this is the central air-conditioning of the residences and its varying usage during the year. A more detailed explanation follows now. First of all, we already know from exercise 1.2 that most of the residences of our data set have a central air-conditioning. Additionally we mentioned that a huge part of the residential electricity consumption is used for the central air-conditioning. For example, residences in the south atlantic region of the U.S. need $21.2\%$ of the electricity for air-conditioning (according to U.S. Energy Information Administration (2009), link: http://www.eia.gov/consumption/residential/data/2009/index.php#fueluses). Last, you only need air-conditioning during the warmer months of the years and not during the colder ones. 
Hence, the effects of the code change - especially those effects regarding improved efficiency of residential air-conditioning systems - mainly attend the warmer months of the year and lead to a reduced electricity consumption in these months. And this is exactly what the first figure shows.

Now let's turn to the second figure. The results for the natural gas consumption look contrary. The post-code residences consume less natural gas during the winter months in comparison to the pre-code buildings. In all other months of the year the residences built after the code change consume more natural gas. But only $3$ estimates are significant (at the $1\%$-level). These are the estimates for the colder months - January, February and December. They indicate a reduction of the natural gas consumption and reach from $14.3\%$ to $23.6\%$. An explanation for these results is that natural gas is mainly used for (space and water) heating. We already mentioned this relationship in exercise 1.2. The exact numbers you can find on http://www.eia.gov/consumption/residential/data/2009/index.php#fueluses. Additionally we all know that heating is only necessary during the winter months. Hence, the effects of the code change - especially the efficiency gains for heating - cause savings of the natural gas consumption of the residences during the winter months. And this is shown by the second figure.

To sum up, we performed a monthly comparison of the differences in the energy consumption of pre- and post-code residences. We found out that the building code change leads to a reduction of the residential electricity and natural gas consumption - respective for the central air-conditioning and for heating. 

*This exercise refers to the page 13 to 15 and figure 1 and 2 of the paper.*

## Exercise 3.3 -- Possible limitations  

In the last two exercises we observed a downward trend in the energy consumption of the Gainesville residences. We assigned the reduction in the energy consumption to the building code change. But are there other possible reasons for the reduced residential energy consumption instead of the code change? We want to find an answer to this question. 

Jacobsen and Kotchen - the authors of the paper - already tried to avoid this problem of other possible reasons for the decreasing energy consumption by their empirical strategy. To be more precise, they only used data for residences which were built (only) within $3$ years before and $3$ years after the code change. Additionally they set up regression models with uniform specific time effects (we also did in exercise 3.1, part 1) and zip-code specific time effects (compare exercise 3.1, part 2) and the resulting estimates were really close to each other.
<br>But now we will also test for another possible reason for the downward trend in the energy consumption. We consider a further regression model with different explanatory variables than the previous models had. We (will) analyse if there is "a potential time trend independent of the energy-code change" (paper, p.15). 
<br> We estimate the differences in the energy consumption separately for each effective year built (EYB) of the residences. Hence, we regress `EYB` on either `elec` or `gas`. We still use the same control variables as in the previous two exercises. To be more precise, the new regression model - **model 3** -  looks like: $$Y_{it} = \theta \cdot EYB_i + \beta \cdot X_i + v_t + \epsilon_{it}.$$
The new parameters of this model are $EYB_i$ which are $7$ different dummy variables for the EYB of each residence (from the years $1999$ to $2005$) and $\theta$. This vector is of primary interest for us. It contains the EYB-specific estimates for the differences in the energy consumption. 

We want to find out if the estimates of the energy consumption for different effective year builts ($\hat\theta$) show a downward trend. 

But after performing this regression analysis we find out that all resulting estimates are not significant and we cannot observe a downward trend. So, we do not let this regression run by ourselves.

To sum up, we considered a potential limitation of our empirical strategy and searched for other possible reasons for the reduction in the residential energy consumption instead of the building code change. But we did not find some other reason.
<br> Hence, "we can conclude that our estimates of the energy-code change on residential energy consumption are not simply capturing an unobserved time trend. Instead, the pre- and post-code-change comparisons appear to be capturing how the more stringent energy code causes a real decrease in residential electricity and natural gas consumption" (paper, p. 16).

*This exercise refers to the page 15, 16 and figures 3 and 4 of the paper.*


## Exercise 4 -- Empirical analysis: difference-in-differences estimation

In this exercise we turn to the second empirical strategy - a difference-in-differences estimation - to determine the effects of the code change on the residential energy consumption. We now primary focus on the relationship of the weather variability and the energy consumption. To be more precise, we analyse if and how changes in the weather differentially influence the energy consumption of pre- and post-code residences.  
"differences in the responsiveness of energy consumption to variability in weather" (paper, p.23)

The structure of this exercise is the following: first, you get a theoretical background, meaning you get a lot of information about the variables we use and additionally about the estimation and regression method. Then we introduce the regression model and perform the estimation with our data. We again divide the regression analysis in two parts, part a) deals with the electricity consumption and part b) with the natural gas consumption of our residences. 

Before we have a focus on the estimation methods which we will use in this exercise, I will give some more information about the weather variables of this data set.
<br>The changes in the weather are measured by the two weather variables **AHDD** (average heating degree days) and **ACDD** (average cooling degree days). We already considered these variables in exercise 1.3. I will shortly repeat the most important facts of these variables. But if it is too little information for you, please take again a look at exercise 1.3. The weather data has been collected by one weather station and it has been averaged to monthly values. The two variables are called `HDD` and `CDD` in our data frame. We plotted the data and found a high variability of the weather variables. We additionally saw that ACDD is much higher than AHDD and that ACDD peaks in the summer months and during the winter it is really close to zero. While AHDD behaves exactly the opposite. Furthermore, we know that the variability of the weather variables is the main reason of fluctuations in the residential energy consumption.  

Now we describe the two estimation methods which we will use in this exercise. These methods are called **difference-in-differences estimation** and **fixed effects regression**. We did not use one of this methods in the previous exercises of this problem set, so I will now give detailed information about them. Finally, we will be able to apply this knowledge to our data and determine the effects of the code change from a different perspective. 

First of all you get some information about the difference-in-differences estimation method in the following **info box**.

#< info "difference-in-difference estimation"
Let's assume we are in the following situation of a "natural experiment, in which one group has experienced the treatment, whereas another, comparable group has not" (Kennedy (2008), p. 382). Then we can use this method to estimate the causal effect of the treatment (according to Stock and Watson (2007), p. 480). The difference-in-difference estimator is the difference between the average changes in $Y$ (the response variable) in/of the two groups - the treatment and control group - over the same time (according to Kennedy (2008), p. 382 and Stock and Watson (2007), p. 480f.). The exact formula of the difference-in-differences (DiD) estimator looks like:  $$\hat\beta^\textrm{DiD}_1 = (\overline{Y}^\textrm{treatment,after} - \overline{Y}^\textrm{treatment,before}) - (\overline{Y}^\textrm{control,after} - \overline{Y}^\textrm{control,before}) = \Delta\overline{Y}^\textrm{treatment} - \Delta\overline{Y}^\textrm{control}$$ with $\overline{Y}^\textrm{treatment,after}$ being the sample average of $Y$ in the treatment group before the experiment and $\Delta\overline{Y}^\textrm{treatment}$ being the average change in $Y$ in the treatment group. The other averages are analogously defined (Stock and Watson (2007), p. 481). If you like further information about the difference-in-differences estimation take a look at Stock and Watson (2007), p. 480-484. 
#>

Now we adapt the difference-in-differences estimator to our data. First, we define the pre-code residences, that are the residences which were built within three years before the code change as **control group** and the post-code residences which were constructed within three years after the code change as **treatment group** (refer to paper, p.23). Second, we note that our data only contain the monthly utility bills for households in Gainesville for the years $2004$ to $2006$. That means the billing records are only collected within the years after the code change.
<br> Hence, we have $36$ time periods instead of $2$ - which is the typical number of time periods for difference-in-differences estimation. But the greater number of time periods causes no problems. We can estimate the coefficients by using a **fixed effects regression model** (according to Stock and Watson (2007), p. 483f. and p. 517).  

Now you get more detailed information about the fixed effects estimation. First, we state why we use fixed effects in our data and then we explain it. In the first **info box** you find some general definitions and then you get a definition of the fixed effects regression model and of the fixed effects estimator. Last, you find an **info box** about the fixed effects regression model to deepen your knowledge.

In the two regression models of this exercise - **model 4** and **model 5** - we use residence-specific intercepts as fixed effects. The aim of these intercepts is to control for "unobserved, time-invariant heterogeneity among residences" (paper, p. 17). 
<br> In the following part, you get a detailed explanation. But first you can take a look at the **info box** and get some basic definitions.   

#< info "definitions - panel data, cross-sectional data and heterogeneity"
This info box provides some basic definitions and explanation about **panel data**, **cross-sectional data** and about the term **heterogeneity**. 
 
Our underlying data is **panel data**. This means we have "data for multiple entities in which each entity is observed at two or more time periods" (Stock and Watson (2007), p. 13). 
<br> We later also mention **cross-sectional data** or cross-sectional units. The difference between panel data and cross-sectional data is that cross-sectional data only contains observations of different entities for one time period and not for two or more time periods (refer to Stock and Watson (2007), p. 11 and 15).

Another important term is **heterogeneity**. It means "that these micro units [of panel data] are all different from one another in fundamental unmeasured ways" (Kennedy (2008), p. 282).
<br> To be more precise, we have cross-sectional heterogeneity in our data - in our case we have heterogeneity among residences. This means, in a plot of (some of) the data we would observe the following situation: the observations of one residence would lie on one line, but if we consider several residences all lines would have the same slope but different intercepts. Hence, we would observe parallel lines, one line per residence. The reason for the different intercepts are unobserved variables that influence the response variable - in our case it is the energy consumption (Kennedy (2008), p. 282f.). For example, in our situation there could be "an unobserved trend in average energy consumption based on EYB (effective year built)" (paper, p.17). We already considered this possible influence in exercise 3.3.

*also show plot with our data - to prove it?*
#>

We already mentioned in the **info box** that the heterogeneity in our data comes from unobserved variables that influence the energy consumption and are correlated with other explanatory variables. Hence, the general problem are **omitted variables** which cause our OLS estimate to be biased (according to Kennedy (2008), p. 283). To be more exact, we have omitted variables that "vary across entities but do not change over time" (Stock and Watson (2007), p. 356), thus they are **time-invariant**. To avoid this problem we include dummy variables for each residence (according to Stock and Watson (2007), p. 356 and Kennedy (2008), p. 282f. and 290). 
<br> To sum up, "fixed effects regression is a method for controlling for omitted variables in panel data when the omitted variables vary across entities but do not change over time" (Stock and Watson (2007), p. 356). You can shortly describe the fixed effects regression model as a regression model that contains a different intercept for each entity - in our case for each residence. Each intercept can be represented by one dummy variable. "These binary variables absorb the influence of all omitted variables" (Stock and Watson (2007), p. 356). "The key insight is that if the unobserved variable does not change over time, then any changes in the dependent variable must be due to influences other than these fixed characteristics" (Stock and Watson (2007), p. 372.).

Hence, a fixed effects estimator is an OLS estimator "applied to the fixed effects model" (Kennedy (2008), p. 283). The fixed effects estimator is also called within estimator because this "estimator uses variation within each cross-sectional unit" (Kennedy (2008), p. 286).

Detailed information about the fixed effects regression model you find in the **info box** below.

#< info "fixed effects regression model"
This info box provides some more information about the fixed effects regression model. You already got some basic knowledge but now you will get more detailed information.

We start with an ordinary regression model: $$Y_{it} = \beta_0 + \beta_1 \cdot X_1 + \epsilon_{it}.$$ The index $i=1,...,n$ refers to entity (in our case: it's the index of the residence) and the index $t=1,...,T$ refers to time (in our case: it's the month-year of the billing record). For simplicity we only use a model with one regressor $X_1$. 

Then we include the (unobserved) variable $Z_i$ which varies between the different residences but not over time. In our case $Z_i$ could be an " unobserved trend in average energy consumption based on EYB" (paper, p.17). Hence, the new regression model looks like: $$Y_{it} = \beta_0 + \beta_1 \cdot X_1 + \beta_2 \cdot Z_i + \epsilon_{it}.$$ Our aim is to estimate $\beta_1$ (more precise: the effect of $X_1$ on $Y$, holding $Z$ constant). 

We can transform this new model to a model which has $n$ different intercepts (in our case: one intercept for each residence): $$Y_{it} = \beta_1 \cdot X_1 + \alpha_i + \epsilon_{it} ~~\textrm{with}~~ \alpha_i = \beta_0 + \beta_2 \cdot Z_i.$$ The transformed model is called **fixed effects regression model** with the (unknown) $\alpha$'s as residence-specific intercepts. We also want to estimate them. (The $\alpha$'s are called residence-specific intercepts because the slope $\beta_1$ of the regression line is the same for all residences but the intercepts are different for each residence. The reason for the variation is the omitted variable - in our case: $Z_i$.) 

There is another, but equivalent method to write the fixed effects regression model: we include dummy variables for each residence (instead of the residence-specific intercepts). We have to be careful because our regression model already includes a common intercept. To avoid perfect multicollinearity of the regressors we cannot include all n dummy variables. Hence, we have to omit one arbitrary dummy variable, we choose the first one. So this model looks like: $$Y_{it} = \beta_0 + \beta_1 \cdot X_1 + \gamma_2 \cdot D2_i + \gamma_3 \cdot D3_i + ... + \gamma_n \cdot Dn_i + \epsilon_{it}$$ with the dummy variables $D1_i$,...,$Dn_i$, $D1_i = 1$, if $i=1$ and $0$ otherwise, $D2_i = 1$, if $i=2$ and $0$ otherwise and so on. Now we can estimate the coefficients by OLS estimation. The estimation gets easier if you transform the data and subtract the corresponding averages. Then you avoid the estimation of all dummy variables. 
<br> In this exercise we will use the function `felm()` for performing the OLS estimation. Hence, we have no problem to include the dummy variables because we will project them out. 

When we compare both models of the fixed effects regression we find the following relationship: $\alpha_1 = \beta_0$ and for all $i \ge 2$ holds $\alpha_i = \beta_0 + \gamma_i$. 

If we have more than two time periods or unobserved variables that vary over time and are not constant, we can expand our model and include further dummy variables - so called time fixed effects. You can get more information in Stock and Watson (2007), chapter 10.4 (p. 361 - 364).

On the opposite, there are also disadvantages of this model. One disadvantage is the loss of degrees of freedoms by the inclusion of the dummy variables. You can read more in Kennedy (2008), p. 283f.

*references: Stock and Watson (2007), p. 356-359 and p. 372, Kennedy (2008), p. 282f. and 292f.*
#>


Now you have a close insight in the theory/background of the difference-in-differences estimation method and the fixed effects regression model. Hence, we can start with the analysis of our billing data of the Gainesville residences. As a reminder, we want to determine the effects of the code change on the residential energy consumption with the main focus on the relationship of the weather variability and the energy consumption. To be more precise, we analyse the "differences in the responsiveness of energy consumption to variability in weather" (paper, p. 23). Thus, we consider the following regression model which we denote by **model 4**: $$Y_{it} = \beta \cdot [CDD_t,HDD_t] + \delta \cdot code\_change_i \times [CDD_t,HDD_t] + month_t + year_t + \mu_i + \epsilon_{it}.$$ 
We already know five of the parameters: first, the weather variables $CDD_t$ and $HDD_t$, second the variable $code\_change_i$ which indicates that the residence was built after the code change and last the two time parameters $month_t$ and $year_t$ which are transformed into dummy variables. An unknown parameter is $\mu_i$ which is a residence-specific intercept. Hence, it is the fixed effect of our fixed effects regression and therefore "controls for any unobserved, time-invariant heterogeneity among residences" (paper, p. 17).
<br> When we estimate this model we again use clustered standard errors - clustered at the residence level, i.e. by the variable `home_id`. As a result, we get estimates for the coefficients $\beta$ and $\delta$. The $\beta$'s are estimates for the uninteracted weather variables and thus apply to the pre-code residences. As opposed to these estimates, the $\delta$'s are the coefficients of the interacted weather variables (with the variable $code\_change_i$) and hence apply to the post-code residences. In our data frame, we denote the interacted variables by `CodeCDD` and `CodeHDD`. The coefficient estimates of these variables are of primary interest for us and to be more precise, they are difference-in-differences estimates and thus indicate "how residences before and after the energy-code change differ in their energy consumption responses to changes in weather" (paper, p. 17).
<br> We expect a difference between the energy consumption responses of pre-code and post-code residences towards changes in weather (paper, p. 17). 

Now we start with the analysis of our data. Therefore, we have to load the data. The data frame is called `DiD_regression.dta`. This data frame consists of $11$ different columns which contain information about the energy consumption, the weather situation and the time and the two variables `home_id` and `code_change`. Enter your commands and then click on `check`. But first of all press the button `edit`.

```{r "4"}
#< task
# load the necessary package
# enter your code here...
#>
library(foreign)
#< hint
display("The required package is called foreign. Load this package with the command library().")
#>

#< task
# read data set DiD_regression.dta into R and assign it to dat
# enter your code here...
#>
dat = read.dta("DiD_regression.dta")
#< hint
display("Your command should look like: dat = read.dta(\"???\").")
#>
```
If you like to see parts of the data frame `dat` click on the button `data` and then the **Data Explorer** will open.

We first analyse the effect of the code change on the electricity consumption and then in part b) we focus on the natural gas consumption. Last we compare all results and draw a conclusion.

### a) Electricity consumption

In this part of the exercise we determine the difference in the electricity consumption responses of pre- and post-code residences towards variability in weather. Hence, our response variable $Y_{it}$ is `elec`.
<br>Before we start with the analysis we will state our expectations towards the results, provided two facts hold: first, the code change really has an effect concerning an increasing energy efficiency and second, the "variability in weather is known to be the primary determinant of changes in residential demand for space heating and cooling, which are the primary end-uses targeted by residential energy codes" (paper, p. 18). 
<br> Then we perform the regression analysis and interpret the results. At the end, we can compare our expectations with the results and draw a conclusion.

Generally, we expect a difference in the responses of the pre- and post-code residences. Regarding electricity consumption, "we would expect **residences constructed after the code change to be less responsive to increases in ACDD**" (paper, p. 18). The reason for this expectation is the following: more ACDD means that households use a lot more air-conditioning but the central air-conditioning of post-code residences is much more efficient because of the code change. Hence, post-code residences are less responsive to raising ACDD. 

Now we perform the regression analysis. We use the function `felm()` of the package `lfe` to estimate the coefficients. We are only interested in the coefficients of the weather variables `CDD` and `HDD`  and the coefficients of the interacted variables `CodeCDD` and `CodeHDD`. These coefficients were called $\beta$ and $\delta$ in our **model 4**. Thus we project the other variables, which are all dummy variables, out. The dummy variables describe the time (`month` and `year`) and also the residence (`home_id`) because we use fixed effects estimation. We additionally use the variable `home_id` in the last part of the formula for clustering of our standard errors. 
<br> The R code is already given. Just press `check` to run the regression which is called `reg4_1` and show its results. 
```{r "4__2"}
#< task
# load the package
library(lfe)

# run the regression
reg4_1 = felm(elec ~  CDD + HDD + CodeCDD + CodeHDD | factor(month) + factor(year) + 
              factor(home_id) | 0 | home_id , data=dat) 

# show the results
summary(reg4_1)  
#>
```
Here you see the results of the regression. We are mainly interested in the $4$ coefficient estimates. All of them are significant.
<br> First, we consider the uninteracted weather variables `CDD` and `HDD`. Both estimates are positive which indicate that an increase of ACDD or AHDD leads to a higher electricity consumption. "These results are all consistent with electricity being the primary energy source for cooling and heating of Florida residences" (paper, p. 18). To be more precise, air-conditioning accounts $6.7\%$ of the electricity consumption of housholds in Florida. Space heating accounts for $5.8\%$ and water heating for $6.2\%$ (according to U.S. Energy Information Administration (2009), link: http://www.eia.gov/consumption/residential/data/2009/index.php#fueluses).  
<br> Second, we take a look at the interacted variables `CodeCDD` and `CodeHDD`. The two coefficients have different signs. The coefficient estimate of `CodeCDD` is roughly $-2.5$ and thus shows that "electricity consumption of post-code-change residences is less responsive to an increase in ACDD" (paper, p.18). To be more exact, an increase of ACDD by $1$ unit is associated with a $2.5$ kWh per month decrease in electricity consumption of post-code residences. When we compare this result with the result of pre-code residences we can say that it "is a $7.8 \%$ decrease in responsiveness to ACDD relative to the response of pre-code-change residences" (paper, p.18). (calculation: $7.8\% = \frac{2.5}{32.25} \cdot 100 \%$)
<br> On the other hand, the coefficient estimate for `CodeHDD` is approximately $2.4$ which shows an increasing responsiveness of post-code buildings towards rising AHDD. Thus it "suggests that the after-code-change residences are less efficient with respect to electric heating" (paper, p. 18). But this results is not really problematic because "heating degree days are far less frequent in Florida" (paper, p. 18). We already observed this situation in the plot of exercise 1.3    
 
To conclude, the results are conform with our expectations. The pre- and post-code residences differ in their electricity consumption responses and the electricity consumption of post-code residences is by $7.8\%$ less responsive to increases in ACDD compared to pre-code buildings. The regression results are also conformable to our results of exercise 3.2 and again show that the energy code change leads to a higher efficiency of central air-conditioning of the residences (according to paper, p. 18). 


But if you take a look at the estimation results of the original paper (printed in table 4) you will notice that the results for the clustered standard errors are different in comparison to our results. The results in the previous code chunk are a little bit higher. The reason is that R and Stata use slightly different calculation methods. Hence, also the (adjusted) R^2 is higher than in the paper and the t-statistic and p-values are not exactly the same.
<br> It is possible to correct our standard errors to get the same results as Jacobsen and Kotchen. We have to multiply the standard errors by the term: $c = \frac{1}{\sqrt{\frac{N-1} {N-J-1}}}$ (according to https://www.fionaburlig.com/blog/2016/8/16/is-the-file-drawer-too-large and http://www.princeton.edu/~mattg/statar/regressions.html). $N$ is the number of observations and $J$ is the number of clusters which is in our case the number of residences. 
<br>*[The authors of the paper used the Stata function `xtreg` to calculate the fixed effects regression. But in this problem set we perform the calculation with the R function `felm()`. And these two functions don't produce the same errors. But we know that the two estimation functions `felm()` and `areg` (Stata function) calculate the same errors (according to http://www.princeton.edu/~mattg/statar/regressions.html). And according to Fiona Burling (see https://www.fionaburlig.com/blog/2016/8/16/is-the-file-drawer-too-large) we only have to mulitply the `areg`-standard errors by the correction term c to get the same results as using the Stata function `xtreg`.]* 
<br> You can perform the calculations for the standard errors of `reg4_1` by clicking `check`. With the command `reg4_1$cse` we get the clustered standard errors of our regression. We are only interested in the standard errors of the coefficients for `CDD`, `HDD`, `CodeCDD` and `CodeHDD` so we choose the first four entries. 

*This task can be solved optionally.*
```{r "4__3",optional=TRUE}
#< task
# correction of standard errors
N = 64471 # number of observations
J = 2239 # number of residences

# correction term: 
c = 1/(sqrt((N-1)/(N-J-1)))

# multiply standard errors by c
correct_se = c * reg4_1$cse[1:4]
correct_se
#>
```

We introduce a second regression model in this exercise to be able to make comparisons and also to check if the results are robust between the two specifications. 
<br> In this model we only estimate coefficients for the interacted weather variables. We additionally use month-year dummies (`monthyear`) instead of single dummy variables for the billing months and years. We also need dummy variables for the different residences to be able to perform a fixed effects regression. Hence, the regression model which we call **model 5** looks like: $$Y_{it} = \delta \cdot code\_change_i \times [CDD_t,HDD_t] + v_t + \epsilon_{it}.$$ For estimation we also need clustered standard errors.  
<br> Press `check` to perform the regression.

```{r "4__4"}
#< task
# run the regression
reg4_2 = felm(elec ~ CodeCDD + CodeHDD | factor(monthyear) + factor(home_id) | 0 | 
                home_id , data=dat)

# show the results
summary(reg4_2)  
#>
```

The results of `reg4_2` are similar to the results of `reg4_1`. The corresponding estimates have equal signs and the same relation. But the coefficient estimates of **model 5** are even larger, meaning that differences between the pre- and post-code residences are greater than in **model 4**.


### b) Natural gas consumption

In this part of the exercise we determine the difference in the natural gas consumption responses of pre- and post-code residences towards changes in weather. Hence, we use in our regression models the variable `gas` as response variable.
<br>Before we start with the regression analysis we will again firstly state our expectations towards the results.

We already mentioned that we generally expect a difference in the responses of the pre- and post-code residences. Particular regarding natural gas consumption, "we would expect **after-code-change residences to be less responsive to increases in AHDD**" (paper, p. 18). The reason is that households use a lot more natural gas for heating when there are more AHDD. But the heating systems of post-code residences are much more efficient, thus post-code residences are less responsive to raising AHDD. 

**Task:** Perform the regression analysis to determine the difference in the natural gas consumption responses of pre- and post-code residences towards changes in weather according to **model 4**. Use the function `felm()` of the package `lfe` to estimate the coefficients. We are only interested in the coefficients of the variables `CDD`, `HDD`, `CodeCDD` and `CodeHDD`. Hence, project out the other variables  which are all dummy variables. Don't forget to use fixed effects estimation and clustered standard errors at the residence level.
<br> Replace the ??? with the correct commands and uncomment the code. Then press `check`. If you need some advice press `hint`.
```{r "4__5"}
#< task
# run regression
# reg4_3 = ???
#>
reg4_3 = felm(gas ~ CDD + HDD + CodeCDD + CodeHDD | factor(month) + factor(year) + 
              factor(home_id) | 0 | home_id , data=dat) 
#< hint
display("Your code should have the following structure: reg4_3 = felm(??? ~ ??? | factor(month) + factor(year) + factor(home_id) | 0 | home_id , data=dat). Replace the ??? by the correct response and explanatory variables.")
#>
```

#< award "regression master - fixed effects regression"
Great! You performed a fixed effects regression on your own.
#>

Now we show the regression results of `reg4_3`. The R code is already given. Just press `check`. 

```{r "4__6"}
#< task
# show regression results
summary(reg4_3)
#>
```

Here you see the results of the regression. Now we will interpret them. All $4$ coefficient estimates are highly significant.
<br> First, we consider the uninteracted weather variables `CDD` and `HDD`. The estimates have different signs, `CDD` is roughly $-0.27$ and `HDD` is approximately $2.33$. It indicates that natural gas consumption of pre-code residences is decreasing with rising ACDD and on the opposite, the gas consumption is increasing with increasing AHDD. This goes along with our expectations because natural gas is mainly used for space and water heating in residences. You can find suitably data on http://www.eia.gov/consumption/residential/data/2009/index.php#fueluses.  
<br> Second, we take a look at the interacted variables `CodeCDD` and `CodeHDD`. The two coefficient estimates are both negative. The coefficient estimate for `CodeHDD` is approximately $-1.37$ which shows that the natural gas consumption of post-code residences is decreasing in AHDD. When we compare the pre-code residences which the residences built after the code change, we can conclude that the responsiveness of gas consumption towards AHDD differs by roughly $58.8\%$. (calculation: $58.8\% = \frac{1.37}{2.33} \cdot 100 \%$)
<br> On the other hand, the coefficient estimate of `CodeCDD` is roughly $-0.17$. This estimate indicates that an increase of ACDD by $1$ unit is associated with a decrease of $0.17$ therms per month in the natural gas consumption of post-code residences. This coefficient estimate may/might be problematic but it is not, because "the effect is outweighed by the response to AHDD, because natural-gas consumption is much more responsive to one additional heating degree day, than to one less cooling degree day" (paper, p. 19). 

To sum up, "the results suggest that post-code-change residences are more efficient with respect to natural-gas consumption for heating" (paper, p. 19). To be more exact, the natural gas consumption of post-code residences is by $58.8\%$ less responsive to increases in AHDD in comparison to pre-code residences. Hence, we can conclude that the results are conform with our expectations.  

We also perform the regression analysis of **model 5** for natural gas consumption. This regression should be denoted by `reg4_4`. We additionally show the output of the regression but this time in comparison to the results of `reg4_3`. Therefore we use the command `screenreg` out of the package `texreg`.
<br> The code is already given. Press `check` to see the solution.

```{r "4__7"}
#< task
# robustness check
reg4_4 = felm(gas ~ CodeCDD + CodeHDD | factor(monthyear) + factor(home_id) | 0 | 
                home_id , data=dat)

# load the package
library(texreg)

# show regression results of reg4_3 and reg4_4 in one table
screenreg(list(reg4_3, reg4_4), 
          custom.model.names=c("Natural gas (model 4)", "Natural gas (model 5)"),
          omit.coef="(factor)", 
          digits=3, stars = c(0.01, 0.05, 0.10))
# without R-squared and adjusted R-squared?
#>
```
Now you can compare the results of both regressions. Answer the question of the following quiz.

#< quiz "robustness check - natural gas, fixed effects regression"
question: Denote the correct statement.
sc:
    - The two regression models are not robust.
    - The coefficient estimates of both specifications are very similar. Hence, the results are robust.*  
    
success: Great, your answer is correct!
failure: Try again.
#>

<br> The last part of this exercise is a summary of the regression analysis. We shortly compare our expectations with the regression results. Finally, we will draw a conclusion. 
<br>First, we take a look at the results of part a).  
  
expectations with respect to electricity | results | conclusion
-----------------------------------------|---------------|-------------------------------------
`CodeCDD`: post-code residences are less responsive to increases in **ACDD** |-2.498 kWh per month| results and expectations are consistent
`CodeHDD`: less responsiveness of post-code buildings to increases in AHDD |+2.45 kWh per month| not conform, but HDD less frequent in Florida

<br>Second, we consider the results of part b).  
  
expectations with respect to natural gas | results | conclusion
-----------------------------------------|--------------|-------------------------------------
`CodeHDD`: less responsiveness of post-code buildings to increases in **AHDD** |-1.37 therms per month| results and expectations are consistent
`CodeCDD`: post-code residences are less responsive to increases in ACDD |-0.17 therms per month| not conform, but the effect of ACDD is predominated by the effect of AHDD 

<br>Last we can conclude that the central air-conditioning and the heating systems of post-code residences are much more efficient than the ones of pre-code residences. 

*This exercise refers to page 10f. and 16 - 19 and table 4 and figure 5 of the paper.*


## Exercise 5 -- Conclusion

"using only monthly utility bills for the years after the code change" (p.23) - add to introduction (and maybe exercise 1)

<!-- table 5 -->
  p. 4
Costs and benefits - mention shortly


p.23: exercise 3/first empirical strategy produces main results
key identification assumptions 

regarding exercise 4:
  "test whether or not the effect of the code is greatest when demand for space heating and cooling are greatest" (p.10)
expectations: "the effect of the code will be greatest during months when the demand for heating and cooling makes up a relatively greater share of a household's energy demand"; 
reason: "Florida's energy code only regulates energy efficiency related to space heating, space cooling and water heating"
in detail: for electricity: "effect will be greatest in the summer months when electricity demand for air-conditioning is at its peak"; for gas: "effect will be greatest in the winter months when demand for natural-gas based heating is at its peak " (paper, p. 10 f.)


<br> p. 23 "fixed effects models account for average differences in consumption among residences and test for differences in the response to weather variability, the main driver of fluctuations in the energy demand. One interpretation of these models is as a robustness check against the other estimates of monthly differences explained by energy demand due to heating and cooling (means the ones from exercise 3.2?). if the energy-code change increases efficiency, then consistency between approaches would imply that more cooling degree days and heating degree days should be associated with less of an increase in electricity and natural-gas consumption, respectively, for the post-code-change residences. (but why??) This in, in fact, precisely what we find."



## Exercise 6 -- References

**Paper** corresponds to: Jacobsen, Grant D. and Matthew J. Kotchen (2010): "Are Building Codes Effective at Saving Energy? Evidence from Residential Billing Data in Florida." NBER Working Paper 16194. Internet: http://www.nber.org/papers/w16194.pdf.
<br> The corresponding **data** is available on  https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/20533.


### Bibliograhpy

- EnergyGauge (2002): Florida's new 2001 energy code. Accessed on October 28, 2016 at http://www.energygauge.com/FlaRes/new_code.htm .
- EPA (United States Environmental Protection Agency) (2014): Climate Change Indicators in the United States: Heating and Cooling Degree Days. Accessed on October 20, 2016 at https://www3.epa.gov/climatechange/pdfs/print_heating-cooling-2014.pdf.
- Kennedy, Peter (2008): "A Guide to Econometrics". Sixth Edition. Malden, Massachusetts [i. a.]: Blackwell Publishing  Ltd.
- Lu, Xun and Halbert White (2013): "Robustness Checks and Robustness Tests in Applied Economics." Journal of Econometrics 178, 194 - 206. Internet: http://www.sciencedirect.com/science/article/pii/S0304407613001668. 
- Stock,  James H. and  Mark W. Watson (2007): "Introduction to Econometrics." Second Edition. Boston: Pearson Education, Inc. 
- U.S. Energy Information Administration (2009): Table HC1.10 Fuels Used and End Uses in Homes in South Region, Divisions, and States. Accessed on February 2, 2017 at http://www.eia.gov/consumption/residential/data/2009/index.php#fueluses.
- Weerahandi , Samaradasa (1995): "Exact Statistical Methods for Data Analysis." New York [i. a.]: Springer-Verlag.


### R packages and functions

#### General R(?)

- Burling, Fiona (2016): "Is the file drawer too large? Standard Errors in Stata Strike Back". Accessed on January 7, 2017 at https://www.fionaburlig.com/blog/2016/8/16/is-the-file-drawer-too-large.
- Kranz, Sebastian (2015): "Developing Interactive R Problem Sets with RTutor". Version from 2015-05-26. Accessed at https://github.com/skranz/RTutor/blob/master/vignettes/Guide_for_Developing_Interactive_R_Problemsets.pdf. 
- Kranz, Sebastian (2016): "An RTutor Problem Set as Bachelor or Master Thesis - A Guide". Version from 2016-08-12. Accessed at https://github.com/skranz/RTutor/blob/master/vignettes/An_RTutor_Problem_Set_as_Bachelor_or_Master_Thesis.pdf. 
- Maindonald, John and John Braun (2007): "Data Analysis and Graphics Using R - an Example-Based Approach." Second Edition. Cambridge [i. a.]: Cambridge University Press.

#### R packages

-"dplyr": Wickham, Hadley and Romain Francois (2016): "A Grammar of Data Manipulation". R package version 0.5.0. Accessed on October 18, 2016 at https://cran.r-project.org/web/packages/dplyr/dplyr.pdf.
-"foreign": R Core Team (2016): "Read Data Stored by Minitab, S, SAS, SPSS, Stata, Systat, Weka,
dBase, ...". R package version 0.8-67. Accessed on October 4, 2016 at https://cran.r-project.org/web/packages/foreign/foreign.pdf.
- "ggplot2": Wickham, Hadley and Winston Chang (2016): "Create Elegant Data Visualisations Using the Grammar of Graphics". R package version 2.2.1. Accessed at https://cran.r-project.org/web/packages/ggplot2/ggplot2.pdf.  
- "ggplot2": R Studio: "Daten veranschaulichen mit ggplot 2". Accessed at https://www.rstudio.com/wp-content/uploads/2015/06/ggplot2-german.pdf. (first access on September 8, 2016)  
-"ggplot2": Accessed on http://docs.ggplot2.org/0.9.3.1/index.html. (first access on September 20, 2016)
- "ggplot2": ZevRoss (2014, update 2016): "Beautiful plotting in R: A ggplot2 cheatsheet". Accessed at http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/. (first access on September 20, 2016)  
-"gridExtra": Auguie, Baptiste and Anton Antonov (2016): "Miscellaneous Functions for "Grid" Graphics". R package version 2.2.1. Accessed on October 14, 2016 at https://cran.r-project.org/web/packages/gridExtra/gridExtra.pdf.  
-"lfe": Gaure, Simen (2016): "Linear Group Fixed Effects". R package version 2.5-1998. Accessed on November 21, 2016 at https://cran.r-project.org/web/packages/lfe/lfe.pdf.  
-"RTutor": Kranz, Sebastian (2015): "R problem sets with automatic test of solution and hints". R package version 2015.12.16. Accessed at https://github.com/skranz/RTutor.  
- "texreg": Leifeld, Philip (2016): "Conversion of R Regression Output to LaTeX or HTML Tables". R package version 1.36.18. Accessed on December 5, 2016 at https://cran.r-project.org/web/packages/texreg/texreg.pdf. 
-"tidyr": Wickham, Hadley (2017): "Easily Tidy Data with "spread()" and "gather()" Functions". R package version 0.6.1. Accessed on November 14, 2016 at https://cran.r-project.org/web/packages/tidyr/tidyr.pdf.

#### R functions

-"confint": R Core Team (2017?): "Confidence Intervals for Model Parameters". R package "stats" version 3.4.0. Accessed on December 7, 2016 at https://stat.ethz.ch/R-manual/R-devel/library/stats/html/confint.html.  
-"factor": R Core Team (2017?): "Factors". R package "base" version 3.4.0. Accessed on November 21, 2016 at https://stat.ethz.ch/R-manual/R-devel/library/base/html/factor.html.   
-"felm": Gaure, Simen (2016): "Fit a linear model with multiple group fixed effects". R package "lfe" version 2.5-1998. Accessed on November 21, 2016 at http://finzi.psych.upenn.edu/R/library/lfe/html/felm.html.  
-"interaction": R Core Team (2017?): "Compute Factor Interactions". R package "base" version 3.4.0. Accessed on November 23, 2016 at https://stat.ethz.ch/R-manual/R-devel/library/base/html/interaction.html.  
-"legends". Package "ggplot2"(?). Accessed at http://www.cookbook-r.com/Graphs/Legends_(ggplot2)/. (first access on 22.09.2016)  
-"seq.Date": R Core Team (2017?): "Generate Regular Sequences of Dates". R package "base" version 3.4.0. Accessed on September 21, 2016 on https://stat.ethz.ch/R-manual/R-devel/library/base/html/seq.Date.html.   
-"t.test": R Core Team (2017?): "Student's t-Test". R package "stats" version 3.4.0. Accessed on November 15, 2016 on https://stat.ethz.ch/R-manual/R-devel/library/stats/html/t.test.html.  

